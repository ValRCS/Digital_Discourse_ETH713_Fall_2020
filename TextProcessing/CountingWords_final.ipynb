{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment: Count Most Frequently used words in Veidenbaums.txt\n",
    "# For English let's use Alice in Wonderland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open File - Get Data\n",
    "# Read Text\n",
    "# Split Text into word tokens\n",
    "# Count these tokens (we need to figure out how to)\n",
    "# Save/Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File is under /data/Veidenbaums.txt\n",
    "# Alice is under /data/Alice_Wonderland.txt\n",
    "# we are under /TextProcessing/CountingWords.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means one level up and then again down into data\n",
    "# \"../data/Veidenbaums.txt\"\n",
    "# So called relative path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164410"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filePath = \"../data/Veidenbaums.txt\"\n",
    "# .. means one level up\n",
    "# data folder is a sibling of TextProcessing\n",
    "filePath = \"../data/Alice_Wonderland.txt\"\n",
    "# with is so called context manager\n",
    "# it will automatically close the file - otherwise you need to do it manually - which everyone forgets to do :)\n",
    "with open(filePath, encoding=\"utf-8\") as fstream: # openign a filestream\n",
    "    mytext = fstream.read() # here our textual data is read into memory\n",
    "    # as with most things in Python after : you need to indent\n",
    "    # fstream is still open here\n",
    "    # there are other ways to read files - but this is the most common one\n",
    "    # who said that you can't step into the same river twice?\n",
    "    # you can - but it will be a different river\n",
    "    # which Greek philosopher said that?\n",
    "    # Heraclitus - one of the pre-Socratics\n",
    "    # with file streams we can reset the pointer to the beginning of the file\n",
    "    # fstream.seek(0) - rare that you would need to do this\n",
    "    # usually you read the file once and then do something with it - like count words\n",
    "    # you do not need to read the file again - maybe you will want to write it again but that is a different story\n",
    "# once indentation ends here fstream is closed automatically for you\n",
    "len(mytext) # how many symbols are in our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Alice’s Adventures in Wonderland, by Lewis Carroll\\n\\nThis eBook is for the use of anyone '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytext[:120] # first 120 symbols from our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-tm,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytext[-200:] # last two hundred symbols of our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg EBook of Alice’s Adventures in Wonderland, by Lewis Carroll\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and most\n",
      "other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever.  You may copy it, give it away or re-use it under the terms o\n"
     ]
    }
   ],
   "source": [
    "print(mytext[:300]) # print first 300 symbols\n",
    "# print gives your more human readable output than just typing the variable name - which is more suitable for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so one issue is that pretty much any data source will need additional cleaning\n",
    "# here we have header and footer sections with some legal data \n",
    "# this meta-data would distort our analysis\n",
    "# for one we would get mentions of Mr. Gutenberg \n",
    "# which Mr. Lewis Carrol did not intend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type of mytext is string\n",
    "type(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145459"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we need to do some cleaning first\n",
    "# ideas on getting rid of header and footer sections\n",
    "#\n",
    "mytext.index(\"End of Project Gutenberg\") \n",
    "#this will the index of the first mention of the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164410"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so there are about 17000 symbols between the end of the book and the end of the file\n",
    "# we can use this to get rid of the footer\n",
    "# we can also use this to get rid of the header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145459"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so cleanend will be a new string containing only the text of the book from the start of the book to the end of the book\n",
    "# we use slicing to get the text we want\n",
    "cleanend = mytext[:mytext.index(\"End of Project Gutenberg\")] \n",
    "\n",
    "# of course this assumes we know the sentinel value for our end of the book\n",
    "# so this will cut and save text to last character\n",
    "# before the above text is found\n",
    "len(cleanend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of Wonderland of long ago: and how she\n",
      "would feel with all their simple sorrows, and find a pleasure in all\n",
      "their simple joys, remembering her own child-life, and the happy summer\n",
      "days.\n",
      "\n",
      "THE END \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cleanend[-200:]) # last 200 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we find out where the header ends\n",
    "start_index = cleanend.index(\"START OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we also need to know how many characters to skip\n",
    "skip = len(\"START OF THIS PROJECT GUTENBERG EBOOK\")\n",
    "skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ALICE’S ADVENTURES IN WONDERLAND ***\n",
      "\n",
      "\n",
      "\n",
      "Produced by Arthur DiBianca and David Widger\n",
      "\n",
      "[Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Alice’s Adventures in Wonderland\n",
      "\n",
      "by Lewis Carroll\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION 3.0\n",
      "\n",
      "Contents\n",
      "\n",
      " CHAPTER I.     Down the Rabbit-Hole\n",
      " CHAPTER \n"
     ]
    }
   ],
   "source": [
    "cleanall = cleanend[start_index+skip:] # so we want everything from the index+skip\n",
    "print(cleanall[:250]) # print first 250 characters of our cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will open/create a new file with name Alice_cleaned.txt\n",
    "# notice it is in the same folder as current notebook\n",
    "# mode is w - which means write\n",
    "# encoding is utf-8 - we might have some non-ascii characters\n",
    "# file_out is arbitrary name for our filestream object\n",
    "with open('Alice_clean.txt', mode=\"w\", encoding=\"utf-8\") as file_out:\n",
    "    file_out.write(cleanall)\n",
    "# so this recipe will create/overwrite 'Alice_clean.txt'\n",
    "# using utf-8 encoding\n",
    "# will write ALL of the text that is in cleanall variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i could also save the file in a different folder in this case like data which is our sibling folder\n",
    "with open('../data/Alice_clean.txt', mode=\"w\", encoding=\"utf-8\") as file_out:\n",
    "    file_out.write(cleanall)\n",
    "# so this recipe will create/overwrite 'Alice_clean.txt' in data\n",
    "# using utf-8 encoding\n",
    "# will write ALL of the text that is in cleanall variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to create a folder if it does not exist?\n",
    "# we can use pathlib\n",
    "# pathlib is a new module in Python 3\n",
    "# it is a bit more advanced than os\n",
    "\n",
    "# we can use pathlib to create a folder if it does not exist\n",
    "# we can also use it to check if a folder exists\n",
    "\n",
    "from pathlib import Path # import Path from pathlib module\n",
    "# we can create a path object\n",
    "\n",
    "# create folder my_folder if it does not exist\n",
    "Path(\"my_folder\").mkdir(exist_ok=True) # exist_ok=True means do not throw an error if folder exists\n",
    "# in this case it will be a child folder of current folder\n",
    "# next time i run it it will not throw an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ALICE’S ADVENTURES IN WONDERLAND ***\\n', '\\n', '\\n', '\\n', 'Produced by Arthur DiBianca and David Widger\\n', '\\n', '[Illustration]\\n', '\\n', '\\n', '\\n', '\\n', 'Alice’s Adventures in Wonderland\\n', '\\n', 'by Lewis Carroll\\n', '\\n', 'THE MILLENNIUM FULCRUM EDITION 3.0\\n', '\\n', 'Contents\\n', '\\n', ' CHAPTER I.     Down the Rabbit-Hole\\n']\n"
     ]
    }
   ],
   "source": [
    "# lets pretend i do not have cleanall in memory\n",
    "# i can load text from my clean file\n",
    "# mode r is read which is default so not really needed\n",
    "with open('../data/Alice_clean.txt', mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    # mytext = file.read() # so reads everything\n",
    "    # alterantively we can read line by line\n",
    "    # this is useful if we have a very large file\n",
    "    # we can read line by line and process it\n",
    "    text_lines = file.readlines() # this will read all lines into a list\n",
    "# print first 10 lines\n",
    "print(text_lines[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so let's see about tokenization \n",
    "# one is to use split using white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick brown fox    jumped \t over the   \n",
      " \n",
      " sleepy dog\n"
     ]
    }
   ],
   "source": [
    "mysentence = \"A quick brown fox    jumped \\t over the   \\n \\n sleepy dog\"\n",
    "# we have some newlines and some tabs in our sentence\n",
    "print(mysentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'sleepy', 'dog']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python string has a split method which will split a string into a list of strings\n",
    "mysentence.split()  # so we split by white space, including newlines and tabs\n",
    "# you can consider split a type of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'jumped',\n",
       " '\\t',\n",
       " 'over',\n",
       " 'the',\n",
       " '',\n",
       " '',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'sleepy',\n",
       " 'dog']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysentence.split(\" \") # so we split by single white space, this might not be what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26537"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could try splitting already but we will get dirty data(words)\n",
    "mywords = cleanall.split() # so any white space will be a delimiter\n",
    "len(mywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALICE’S',\n",
       " 'ADVENTURES',\n",
       " 'IN',\n",
       " 'WONDERLAND',\n",
       " '***',\n",
       " 'Produced',\n",
       " 'by',\n",
       " 'Arthur',\n",
       " 'DiBianca',\n",
       " 'and',\n",
       " 'David',\n",
       " 'Widger',\n",
       " '[Illustration]',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'THE',\n",
       " 'MILLENNIUM',\n",
       " 'FULCRUM',\n",
       " 'EDITION',\n",
       " '3.0']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mywords[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we need to get rid of some \\n which split did not\n",
    "# so how could we do this?\n",
    "# replace to rescue!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ALICE’S ADVENTURES IN WONDERLAND ***    Produced by Arthur DiBianca and David Widger  [Illustration'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = mytext.replace(\"\\n\", \" \") # why not nothing \"\"?\n",
    "# replacing with \"\" we run the risk of combining words across lines\n",
    "clean_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BOB’S ADVENTURES IN WONDERLAND ***\n",
      "\n",
      "\n",
      "\n",
      "Produced by Arthur DiBianca and David Widger\n",
      "\n",
      "[Illustration]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remember replace is a string method it returns a new string\n",
    "# so we need to assign it to a variable\n",
    "# we can use it to replace multiple characters or strings\n",
    "# we can also use it to replace with nothing\n",
    "\n",
    "# for example\n",
    "bob_adventures = cleanall.replace(\"ALICE\", \"BOB\").replace(\"Alice\", \"Bob\")\n",
    "# so for example we can replace all mentions of Alice with Bob\n",
    "print(bob_adventures[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count Bob\n",
    "bob_adventures.count(\"Bob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# before we split we might want to normalize our text - make it all lower case\n",
    "# we also might want to remove punctuation\n",
    "# we can use string.punctuation to get a list of punctuation symbols\n",
    "# also we can use string.whitespace to get a list of whitespace symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALICE’S',\n",
       " 'ADVENTURES',\n",
       " 'IN',\n",
       " 'WONDERLAND',\n",
       " '***',\n",
       " 'Produced',\n",
       " 'by',\n",
       " 'Arthur',\n",
       " 'DiBianca',\n",
       " 'and',\n",
       " 'David',\n",
       " 'Widger',\n",
       " '[Illustration]',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = clean_text.split()\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we could do additionalal cleaning by checking for maybe bad words\n",
    "# bad characters\n",
    "# unneeded words such stop words , meaning words which do not contribute to the meaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so how could we count the occurences of each word?\n",
    "# we could use something like a dictionary with words being keys and value being \n",
    "# number of occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ALICE’S', 1),\n",
       " ('ADVENTURES', 1),\n",
       " ('IN', 1),\n",
       " ('WONDERLAND', 1),\n",
       " ('***', 1),\n",
       " ('Produced', 1),\n",
       " ('by', 55),\n",
       " ('Arthur', 1),\n",
       " ('DiBianca', 1),\n",
       " ('and', 718),\n",
       " ('David', 1),\n",
       " ('Widger', 1),\n",
       " ('[Illustration]', 1),\n",
       " ('Alice’s', 11),\n",
       " ('Adventures', 2),\n",
       " ('in', 346),\n",
       " ('Wonderland', 2),\n",
       " ('Lewis', 1),\n",
       " ('Carroll', 1),\n",
       " ('THE', 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict = {}\n",
    "for word in words:\n",
    "    if word in word_dict.keys():\n",
    "        word_dict[word] += 1 # increase count for each occurence\n",
    "    else:\n",
    "        word_dict[word] = 1 # so first occurence you count as 1\n",
    "    # below is just a shorter version of the above\n",
    "#     word_dict[word] = word_dict.get(word, 0) + 1\n",
    "list(word_dict.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1515),\n",
       " ('and', 718),\n",
       " ('to', 706),\n",
       " ('a', 611),\n",
       " ('of', 493),\n",
       " ('she', 485),\n",
       " ('said', 416),\n",
       " ('it', 347),\n",
       " ('in', 346),\n",
       " ('was', 327)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter # python already provides counter!\n",
    "word_count = Counter(words) # give a list of tokens\n",
    "word_count.most_common(10) # and you can get results immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now that we have some prelimenary results in\n",
    "# we could start thinking about stripping stopwords\n",
    "# maybe getting rid of very short words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count[\"she\"] # turns out Counter is just a dictionary with some benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count[\"he\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word_count.tsv\", mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"word\\tcount\\n\") # so we add newlines by hand\n",
    "    for word,count in word_count.most_common():\n",
    "        f.write(f\"{word}\\t{count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use libraries such as CSV and Pandas to take care of writing \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to continue we will need to perform some additional cleaning\n",
    "# maybe think about some visualization of some of this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will need to clean all lines which contain *** as ending characters\n",
    "# so lets try reading lines\n",
    "# filePath = \"../data/Veidenbaums.txt\"\n",
    "# with open(filePath, encoding=\"utf-8\") as fstream:\n",
    "#     mylines = fstream.readlines()\n",
    "# len(mylines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylines[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanlines = [line for line in mylines if line[0]!='\\n']\n",
    "len(cleanlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanlines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not want the lines which end with ***\\n\n",
    "headlines = [line for line in cleanlines if line.endswith(\"***\\n\")]\n",
    "headlines[:5]\n",
    "# we do not need the headlines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not want the lines which end with ***\\n\n",
    "noheadlines = [line for line in cleanlines if not line.endswith(\"***\\n\")]\n",
    "noheadlines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could save the results \n",
    "savePath = \"../data/noHeadVeidenbaums.txt\"\n",
    "with open(savePath, mode=\"w\", encoding=\"utf-8\") as fstream:\n",
    "    fstream.writelines(noheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May 6th lets start with noheadlines\n",
    "myPath = \"../data/noHeadVeidenbaums.txt\"\n",
    "with open(myPath, encoding=\"utf-8\") as fstream:\n",
    "    noheadlines = fstream.readlines()\n",
    "len(noheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "noheadlines = [line for line in noheadlines if not \"Treimanim\" in line]\n",
    "len(noheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceChars = \"\\n-\"\n",
    "stopChars = \"\"\"!?.,\"':;()…\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in stopChars:\n",
    "    print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pēc ideāliem cenšas lielie gari,\\nBet dzīvē ieņemt vietu'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One big text from many lines\n",
    "textNoHead = \"\".join(noheadlines) # we could have used fstream.read earlier\n",
    "textNoHead[:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing \n",
      " with space\n",
      "Replacing - with space\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pēc ideāliem cenšas lielie gari, Bet dzīvē ieņemt vietu pirmie Tie neiespēj'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take off spacy Characters replace with space (why space ? :)\n",
    "for char in spaceChars:\n",
    "    print(f\"Replacing {char} with space\")\n",
    "    textNoHead = textNoHead.replace(char, \" \")\n",
    "#     print(textNoHead[:75])\n",
    "textNoHead[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing ! with nothing\n",
      "Replacing ? with nothing\n",
      "Replacing . with nothing\n",
      "Replacing , with nothing\n",
      "Replacing \" with nothing\n",
      "Replacing ' with nothing\n",
      "Replacing : with nothing\n",
      "Replacing ; with nothing\n",
      "Replacing ( with nothing\n",
      "Replacing ) with nothing\n",
      "Replacing … with nothing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pēc ideāliem cenšas lielie gari Bet dzīvē ieņemt vietu '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for char in stopChars:\n",
    "    print(f\"Replacing {char} with nothing\")\n",
    "    textNoHead = textNoHead.replace(char, \"\")\n",
    "textNoHead[:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = \"../data/noHeadVeidenbaumsOneLine.txt\"\n",
    "with open(savePath, mode=\"w\", encoding=\"utf-8\") as fstream:\n",
    "    fstream.write(textNoHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-cadf8a59266b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtextNoHead\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vēstule\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# nothing found thats good\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "textNoHead.index(\"Vēstule\")\n",
    "# nothing found thats good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eiz zaļoja jaunība cerības pla'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textNoHead[5400:5430]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'G',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'z',\n",
       " 'Ā',\n",
       " 'ā',\n",
       " 'č',\n",
       " 'Ē',\n",
       " 'ē',\n",
       " 'ģ',\n",
       " 'Ī',\n",
       " 'ī',\n",
       " 'ķ',\n",
       " 'ļ',\n",
       " 'ņ',\n",
       " 'Š',\n",
       " 'š',\n",
       " 'ū',\n",
       " 'ž'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# charSet that's Camelcase another style\n",
    "char_set = set(textNoHead)\n",
    "char_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8230"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pēc', 'ideāliem', 'cenšas', 'lielie', 'gari']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = textNoHead.split()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pēc', 'ideāliem', 'cenšas', 'lielie', 'gari']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to convert to lower case\n",
    "# for word in words:\n",
    "words_lower = [word.lower() for word in words]\n",
    "words_lower[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1865"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to do it ourselves\n",
    "# we could store it in a dictionary word and count\n",
    "# {'pēc':5, 'ideālie':1, 'cenšas':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1064"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words = set(words_lower)\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i create a dictionary of unique words and set counter to 0\n",
    "my_counter_dict = {word:0 for word in list(unique_words)}\n",
    "my_counter_dict['pēc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_lower:\n",
    "    my_counter_dict[word] += 1 # each time i add 1 to right box(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_counter_dict['pēc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apkakli', 1), ('šūpulis', 1), ('aša', 1), ('līdz', 8), ('tiesas', 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list_tuples = [(key, value) for key,value in my_counter_dict.items()]\n",
    "my_list_tuples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('acs', 1), ('agrāk', 1), ('aiz', 1), ('aizgāja', 1), ('aizmirsts', 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(my_list_tuples)[:5]\n",
    "# not quite what we need because it sorts by the first item alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('un', 76),\n",
       " ('ir', 24),\n",
       " ('vēl', 22),\n",
       " ('tu', 21),\n",
       " ('tik', 21),\n",
       " ('bet', 15),\n",
       " ('kas', 15),\n",
       " ('nav', 14),\n",
       " ('man', 14),\n",
       " ('kā', 13)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution we pass a function to show how to sort\n",
    "my_most_common = sorted(my_list_tuples,key=lambda mytuple: mytuple[1], reverse=True)\n",
    "my_most_common[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so sorting is possible but my recommendation is to use Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well and now I would to like sort\n",
    "# its possible then I need to create a list from dictionary and then sort by key value\n",
    "# solution use a library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batteries are included no need to write our own counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycounter = Counter(words_lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('un', 76),\n",
       " ('ir', 24),\n",
       " ('vēl', 22),\n",
       " ('tik', 21),\n",
       " ('tu', 21),\n",
       " ('bet', 15),\n",
       " ('kas', 15),\n",
       " ('nav', 14),\n",
       " ('man', 14),\n",
       " ('par', 13)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycounter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mycounter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1283"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to get only words 4 chars or longer ? :)\n",
    "long_words = [word for word in words_lower if len(word) >= 4 ]\n",
    "len(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reiz', 10),\n",
       " ('viss', 9),\n",
       " ('līdz', 8),\n",
       " ('mums', 7),\n",
       " ('sauc', 6),\n",
       " ('gars', 6),\n",
       " ('projām', 6),\n",
       " ('laiks', 5),\n",
       " ('sirds', 5),\n",
       " ('tomēr', 5)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_counter = Counter(long_words)\n",
    "long_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'alus' in long_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(long_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_counter.get('alus'), long_counter['alus'] #2nd would throw error if no beer existed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laiks', 5),\n",
       " ('sirds', 5),\n",
       " ('tomēr', 5),\n",
       " ('likumīgi', 5),\n",
       " ('dzīves', 5),\n",
       " ('iedzer', 5)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only get 5 letter words here\n",
    "word_counter_5 = [mytuple for mytuple in long_counter.most_common() if mytuple[1] == 5]\n",
    "word_counter_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('most_common.json', mode='w', encoding='utf-8') as fstream:\n",
    "    json.dump(mycounter.most_common(), fstream, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to save our Latvian or other languages besides ENglish we set\n",
    "# turn off ascii\n",
    "# https://stackoverflow.com/questions/18337407/saving-utf-8-texts-in-json-dumps-as-utf8-not-as-u-escape-sequence\n",
    "with open('most_common.json', mode='w', encoding='utf-8') as fstream:\n",
    "    json.dump(mycounter.most_common(), fstream, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
