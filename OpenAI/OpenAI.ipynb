{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenAI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing OpenAI library\n",
        "\n",
        "Principles will be very similar to other AI libraries (Mistral, etc)\n",
        "\n",
        "Open AI library is not currently included in Google Colab.\n",
        "I would imagine Google would want to include some of their products(that is libraries to access them).\n",
        "Those are Bard and Gemini (as of December 2023).\n",
        "\n",
        "* Google did most of the original reasearch on Large Language Models in the last 15 years - but has not capitalized on it\n",
        "* Instead companies such as OpenAI have overtaken Google in the mind-share and usability\n",
        "* It is a classical case of Innovators Dilemma - Google was making too much money on their Search and they were afraid to hurt their product, now they have little choice\n",
        "(lots of analogies elsewhere, for example BMW early electrical car looked very different from standard BMW because they were afraid of hurting their main product, now BMW realized that they have to catch up to companies such as Tesle, etc)\n",
        "\n",
        "OpenAI is not immune, there are companies, startups such as Mistral from France, Europe that are very promising and may overtake OpenAI.\n",
        "Competition in this field is high - and OpenAI could use some competition, because OpenAI is not actually open...\n",
        "\n",
        "So pragmatically for now we will be using OpenAI to do our LLM tasks."
      ],
      "metadata": {
        "id": "cPPDxkbtuoRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZnYIGn7bsV8",
        "outputId": "c62f9d57-9449-4e61-c845-d8af8c8a0e32"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "snFB8EfCbE8y"
      },
      "outputs": [],
      "source": [
        "import os # standard Python library for os related tasks\n",
        "import openai # so this is openai library we just installed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting OpenAI API key\n",
        "\n",
        "It used to be that you could get OpenAI key for free at least for a month and you could get some credits. Maybe you still get one month or $10 free credits."
      ],
      "metadata": {
        "id": "z-XBAO89BCMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass # standard library into Python\n",
        "secret = getpass('Enter the secret API key for OpenAI value: ') # so very similar to input function but with stars..."
      ],
      "metadata": {
        "id": "Zf5SaV5Wu7JT",
        "outputId": "0f97de7b-adc6-4076-f64d-cf11de4ad2c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the secret API key for OpenAI value: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# openai.api_key = \"use_your_own_key!\"  #never share your private API keys with the world! read them from enviroment or private text file\n",
        "# or using getpass and copy pasting (like I did - not very convenient but good for one time use)\n",
        "# alternatives, store API keys on your Google Drive - personally I do not recommend\n",
        "# there is also something called Google Secrets, again I personally do not trust it, but your mileage may vary\n",
        "openai.api_key = secret  #never share your private API keys with the world! read them from enviroment or private text file"
      ],
      "metadata": {
        "id": "DVAjoD-bb4Vf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing the API\n",
        "\n",
        "https://github.com/openai/openai-python - documents the changes\n",
        "\n",
        "Since this is cutting edge research, API changes quite often.\n",
        "\n",
        "Expect things to stabilize in a few years."
      ],
      "metadata": {
        "id": "NUcrvN9ivPWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    api_key=secret, # instead of accessing my api key from my os, i use the key i pasted in to secret\n",
        "    # again secret is just a string but do not write it directly here!!\n",
        ")"
      ],
      "metadata": {
        "id": "aFmfUYbFvmWb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Say this is a test but translate into Latvian\", # so this is your prompt\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\", # this is among the cheapest of the models similar to the free version on ChatGPT\n",
        "    # there are more options but we will stick with basics\n",
        ")\n",
        "# so with this example we made a call to OpenAI using the API key"
      ],
      "metadata": {
        "id": "HkelQakVvedo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can get the json response of everything the model provided\n",
        "chat_completion.json"
      ],
      "metadata": {
        "id": "CP9VHpHevzmc",
        "outputId": "30d17c0b-e3fd-412c-d4b8-2762b1a60739",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method BaseModel.json of ChatCompletion(id='chatcmpl-8YbN0rAZoGQbb1kjO6y5crwo04T5x', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='≈†is ir tests.', role='assistant', function_call=None, tool_calls=None))], created=1703257494, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=6, prompt_tokens=18, total_tokens=24))>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now to get just the answer in text form\n",
        "chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "1L3M1pPewD5Q",
        "outputId": "f857a23f-bfd4-45fd-ddf9-e736d97b41fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'≈†is ir tests.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chat_completion.choices) # how many choices do we have?"
      ],
      "metadata": {
        "id": "am4dU3p9wvqr",
        "outputId": "80200cfe-b3ef-4bc5-d475-262cde975ebe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Old API call - for historic purposes - changed in November 2023"
      ],
      "metadata": {
        "id": "OISUWA1AxD7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=\"Social media post: \\\"That new Spider Man movie stinks to high heaven\\\"\\nSentiment (positive, neutral, negative):\",\n",
        "  temperature=0,\n",
        "  max_tokens=1,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")"
      ],
      "metadata": {
        "id": "XuMHtVpVcF-c",
        "outputId": "eba6d7fc-cb74-43a6-d52e-cc97abf1459a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIRemovedInV1",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4a0842a8df05>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = openai.Completion.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"davinci\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Social media post: \\\"That new Spider Man movie stinks to high heaven\\\"\\nSentiment (positive, neutral, negative):\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1XJemgVcTzk",
        "outputId": "6c606106-19b6-47c6-ce12-8376b8409922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"length\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" Negative\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1639577707,\n",
            "  \"id\": \"cmpl-4FPLfIQvBLsWX6ewmmTmkHarFKO8W\",\n",
            "  \"model\": \"davinci:2020-05-03\",\n",
            "  \"object\": \"text_completion\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=\"Social media post: \\\"That new Spider Man movie is decent\\\"\\nSentiment (positive, neutral, negative):\",\n",
        "  temperature=0,\n",
        "  max_tokens=1,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEPHk552c2il",
        "outputId": "2ad4f425-201f-4a69-f8c4-91e6006bf82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8kVgk1keM2I",
        "outputId": "fbd34e56-fbf4-4d88-8d3b-ee2813ad3088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"length\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" positive\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1639577843,\n",
            "  \"id\": \"cmpl-4FPNrGBWvwFN76Eeyb5oIQHufsZ7J\",\n",
            "  \"model\": \"davinci:2020-05-03\",\n",
            "  \"object\": \"text_completion\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jd-lg20kyCZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=\"Social media post: \\\"The first film had a much better balance between story and action. It seemed that this film had tons of unnecessary exposition (story really starts around 40 minutes into the movie), and the action was drawn out with lengthy CGI shots that did nothing to showcase the actors' talents, nothing to advance the story, and at provided little spectacle.\\\"\\nSentiment (positive, neutral, negative):\",\n",
        "  temperature=0,\n",
        "  max_tokens=1,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "Xef4oKEqmTUm",
        "outputId": "da74c9fa-7fde-46d1-80fd-cf7a07d7226a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIRemovedInV1",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-2fe6b0e492e8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = openai.Completion.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"davinci\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Social media post: \\\"The first film had a much better balance between story and action. It seemed that this film had tons of unnecessary exposition (story really starts around 40 minutes into the movie), and the action was drawn out with lengthy CGI shots that did nothing to showcase the actors' talents, nothing to advance the story, and at provided little spectacle.\\\"\\nSentiment (positive, neutral, negative):\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Social media post: \\\"That new Spider Man movie is decent\\\"\\nSentiment (positive, neutral, negative):\"\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt, # so this is your prompt\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\", # this is among the cheapest of the models similar to the free version on ChatGPT\n",
        "    # there are more options but we will stick with basics\n",
        ")\n",
        "chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "yfQCoAhOGhff",
        "outputId": "8a352f9b-4404-41b3-c200-341d91bcdf24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automating Sentiment Analysis\n",
        "\n",
        "To avoid writing all the boileplate code by hand, instead I will write a function that combines all the required code and I can call the function instead, whenever I need sentiment analysis."
      ],
      "metadata": {
        "id": "J3BljXVmHLYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getSentiment(prompt, client=client, sentiments=(\"positive\",\"neutral\",\"negative\"), model=\"gpt-3.5-turbo\", max_prompt=200):\n",
        "    sentiment_text = \",\".join(sentiments)  # I add all the sentiments in a string separated by comma\n",
        "    prompt=f\"Social media post: \\\"{prompt[:max_prompt]}\\\"Sentiment ({sentiment_text}):\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "uqSeqhEcyFY-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"I really like bread and circuses\")\n",
        "# depending on temperature setting we might get different answers"
      ],
      "metadata": {
        "id": "gwvz0Tbvyn2z",
        "outputId": "c92c7061-5c11-4e63-b0e5-629b844e895d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Where do I begin? This is a brand new 4K scan from the original negative of the movie with an added HDR10 & Dolby Vision HDR grading, which looks fantastic!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yrZMnvmhnx1W",
        "outputId": "6edcf723-ab1d-4890-960d-7109e5c56115"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Man patik alus\") # I like beer in Latvian"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Esn2RaidoN_n",
        "outputId": "6f2f0e0f-de3e-4928-88ea-d683c3820110"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neutral'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Man nepatƒ´k slidenas ielas\") #I do not like slippery streets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0ZdW9uicoWXh",
        "outputId": "c23b7b08-3ab3-42c5-b8f2-a5eb4e7694fb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using this custom function you can pass in your own sentiments - for example in Latvian\n",
        "getSentiment(\"Man nepatƒ´k slidenas ielas\", sentiments=[\"pozitƒ´vs\", \"neitrƒÅls\", \"negatƒ´vs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vj0lBNvfpDk4",
        "outputId": "2c5fb68d-3b5e-4c1e-f109-26a2d42a6b89"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negatƒ´vs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Man patik alus\", sentiments=[\"pozitƒ´vs\", \"neitrƒÅls\", \"negatƒ´vs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I1uBEGwQpShR",
        "outputId": "338f1029-315f-489a-a643-ce21d8d47835"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pozitƒ´vs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt creation\n",
        "\n",
        "So basica idea is we provide a sort of answer key when we need sentiment analysis. We ended our prompt with possible answers and LLM gave us one of them."
      ],
      "metadata": {
        "id": "YRWwxTzvIw2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# so this is our own ChatGPT basically, except we can adjust more parameters\n",
        "def getAnswer(prompt, client=client, model=\"gpt-3.5-turbo\", max_prompt=500):\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt[:max_prompt],\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "FWvsAxxhJE-J"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getAnswer(\"What is the capital of Latvia?\")"
      ],
      "metadata": {
        "id": "HPIGW71mJaUy",
        "outputId": "4ffa2731-1f62-4a9a-90cb-11dc06453f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of Latvia is Riga.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# of course LLMs have a infamous tendendcy to hallucinate especially when the model does not have the full answer.\n",
        "# One possible test to evaluate is to ask a question where the answer is between ranges of some information it already knows\n",
        "getAnswer(\"Who was president of Latvia in 1926?\") # key being that 1926 was a year President of Latvia did not do anything special"
      ],
      "metadata": {
        "id": "vmMYuABQJjZQ",
        "outputId": "d0f9629d-b29e-492a-f058-bfde0d8085e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In 1926, the president of Latvia was JƒÅnis ƒåakste.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getAnswer(\"Who was president of Latvia in 1926? Write a short paragraph on this president\") # key being that 1926 was a year President of Latvia did not do anything special"
      ],
      "metadata": {
        "id": "-IPEVFv0KB8q",
        "outputId": "086e5e06-71c9-43b6-c409-2199a5d3bb1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In 1926, the President of Latvia was JƒÅnis ƒåakste. He served as the first President of Latvia from 1922 until his death in 1927. ƒåakste was a prominent lawyer and politician who played a crucial role in the establishment of an independent Latvia. As a representative of the Latvian Farmers' Union, he was elected as the Speaker of the Constitutional Assembly in 1920, which formulated the first Constitution of Latvia. During his presidency, ƒåakste emphasized maintaining constitutional governance, promoting social justice, and improving relations with other countries. However, his presidency was cut short by his untimely passing in 1927, leaving behind a legacy of democratic leadership and dedication to the Latvian nation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### getAnswer as analogous to ChatGPT interface\n",
        "\n",
        "So now if you had say 100 documents to analyse you could write a loop and pass this function 100 times."
      ],
      "metadata": {
        "id": "rS-2_WCJKSes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getMovieEmoji(movie_title,client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"\"\"Back to Future: üë®üë¥üöóüïí\n",
        "    Batman: ü§µü¶á\n",
        "    Transformers: üöóü§ñ\n",
        "    Wonder Woman: üë∏üèªüë∏üèºüë∏üèΩüë∏üèæüë∏üèø\n",
        "    Winnie the Pooh: üêªüêºüêª\n",
        "    The Godfather: üë®üë©üëßüïµüèª‚Äç‚ôÇÔ∏èüë≤üí•\n",
        "    Game of Thrones: üèπüó°üó°üèπ\n",
        "    {movie_title}: \"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "SWXEr_F9qbtH"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iN-UHwVgK-1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(getMovieEmoji(\"The Bourne Conspiracy\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XGO_WoNq6eM",
        "outputId": "bef4bb12-cf9a-4f5b-d178-db4a6f675c1a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üï∂üî´üèÉüèª‚Äç‚ôÇÔ∏è\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getMovieEmoji(\"Top Gun: Maverick\"))"
      ],
      "metadata": {
        "id": "sqqJRsogLgvy",
        "outputId": "6f1108f2-0a4f-4d2c-ec08-a4fc45fdd855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ©üöÄüî•üéñ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getMovieEmoji(\"Frozen 2\"))"
      ],
      "metadata": {
        "id": "jaIIowiALnnO",
        "outputId": "b1474f0f-d943-46ae-940c-1a2341f6589e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üë∏üèªüë∏üèº‚ùÑÔ∏èüå¨Ô∏è\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key takeaway - answers are not deterministic\n",
        "\n",
        "Notice how multiple calls can return different answers, it is due to the temperature setting that slightly changes the answers.\n",
        "\n",
        "Model stays the same but there is an element of slight randomness present.\n",
        "\n",
        "So when you see some amazing LLM examples in action you have to ask how many tries did it take to generate them. Case in point recent Google Gemini Demo presentation."
      ],
      "metadata": {
        "id": "HKSOVVzELvon"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEDcNi-jLudY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.esrb.org/\n",
        "def getGameRating(movie_text,client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"\"\"\"Provide an ESRB rating for the following text:\n",
        "    {movie_text}\n",
        "    ESRB rating:\"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_tokens=60,\n",
        "        temperature=0.7 # temperature ranges from 0 to 1\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "7pIUj35jsAdM"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getGameRating(\"The game opens with a screen in a dark and stormy night. Five gamblers sit around a fire in a dark forest.\")\n",
        "# so one of the issues with LLMs is that you need to provide enought context"
      ],
      "metadata": {
        "id": "-NDcBK7jNJZW",
        "outputId": "12cec7da-b9cc-4faa-c1c1-8f6a7d2b77c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The ESRB rating for the given text would likely be \"Teen\" (T) due to the dark and potentially intense atmosphere depicted in the opening scene.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getMovieRating(movie_text,client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"\"\"\"Provide an MPA rating to the movie based on following description:\n",
        "    {movie_text}\n",
        "    MPA rating:\"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_tokens=60,\n",
        "        temperature=0.7 # temperature ranges from 0 to 1\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "GbR698W1ub-_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getMovieRating(\"They say all happy families are alike but all unhappy families are different in their own way\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IaObBgequjrG",
        "outputId": "c294d0bc-69fe-4e56-e085-8d59a9e57fb8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PG-13'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getStudyNotes(subject, client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"What are some key points I should know when studying {subject} Provide at least five key points\\n\\n1.\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        # using default for now, you can play around with some of the parameters\n",
        "        # temperature=1,\n",
        "        # max_tokens=64,\n",
        "        # top_p=1.0,\n",
        "        # frequency_penalty=0.0,\n",
        "        # presence_penalty=0.0\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "Ti9Vdw62u2AY"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(getStudyNotes(\"Riga\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Aqk-WJTvZh5",
        "outputId": "7ec011da-91b0-4aef-821e-225cf7e9743e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riga is the capital city of Latvia and is located in the Baltic region of Northern Europe.\n",
            "2. With a population of around 700,000, Riga is the largest city in the Baltic states and the third-largest in the Baltic Sea region.\n",
            "3. Riga is known for its well-preserved Art Nouveau architecture, which is recognized by UNESCO as a World Heritage Site.\n",
            "4. The city has a rich history, with influences from various cultures, including German, Russian, and Swedish, due to its strategic location on the Baltic Sea.\n",
            "5. Riga is a major economic and financial center in the region, with a strong service sector, manufacturing industry, and a thriving startup scene.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i am using print because this answer includes newlines\n",
        "study_notes_on_dd = getStudyNotes(\"Digital Discourse\")\n",
        "print(study_notes_on_dd)\n",
        "# note how 1. is missing from the answer?\n",
        "# why?\n",
        "# because I already provide 1. at the end of my prompt\n",
        "# remember LLMs are just word prediction machines (with some useful side effects)"
      ],
      "metadata": {
        "id": "FERP6tSvO9py",
        "outputId": "88250f6d-1230-4bf1-8a26-91eb980706ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digital Discourse refers to the communication and interaction that takes place online or through digital platforms such as social media, chat rooms, and forums.\n",
            "\n",
            "2. It encompasses various forms of communication, including written text, images, videos, emojis, and symbols, that are used to convey messages and engage in conversations online.\n",
            "\n",
            "3. Digital Discourse is often characterized by its informality, speed, and brevity, with users frequently using abbreviations, acronyms, and slang to express themselves quickly and concisely.\n",
            "\n",
            "4. It is influenced by the context in which it takes place, including the platform being used, the intended audience, and the social norms and conventions of that online community.\n",
            "\n",
            "5. Digital Discourse is dynamic and constantly evolving, with new words, phrases, and expressions being coined regularly, and cultural references and memes playing a significant role in shaping online conversations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now i can save my notes to text file\n",
        "with open(\"study_notes.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(study_notes_on_dd)"
      ],
      "metadata": {
        "id": "lXFUi6UFPKOc"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion on use of LLMs in Digital Discourse\n",
        "\n",
        "So LLMs can aid in discourse analysis with what:\n",
        "\n",
        "* Automated Text Analysis\n",
        "* Sentiment Analysis\n",
        "* Topic Modeling\n",
        "* Translation\n",
        "* Context Analysis\n",
        "* Cross-cultural Analyss\n",
        "* Summarization\n",
        "* Bias detection"
      ],
      "metadata": {
        "id": "f1HvTyjRPvp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notes = [getStudyNotes(\"Sentiment Analysis\") for _ in range(5)]  # i ran the same query 5 times, so text completion will be different each tie\n",
        "print(notes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKmTUYYcvkuR",
        "outputId": "0afb34d8-8cd1-4607-aea3-87e00b7e4afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Keywords and phrases can be negative even when they don‚Äôt seem to be\\n2. Popular sentiment analysis is not always the right analysis\\n3. Slang and capitals can make sentences seem negative', ' Semantria tracks sentiment globally by applying sentiment analysis models to content expressed in multiple languages.\\n2. Semantria sentiment analysis models come in three different methods which are Query Based, Feed Based, and Sentiment Match.\\n3. With Query Based sentiment analysis you can input a phrase or sentence for Semantria', \" Find out what is interesting about your business\\n2. Focus on your buying process\\n\\n3. Reduce capital inventory\\n4. Have the temperament of an artists\\n\\nWhat key points do you think I should know for this topic?\\n\\nYou shouldn't study how to do sentiment analysis or focus on this topic if\", ' Authors generally use sentiment words to convey approval versus a negative sentiment\\n a group holds about a person, company, issue, or life events.\\n2. Automated sentiment analysis is a technique for analyzing texts to make scientifically-based assessments on whether a text is of a broadly positive or negative tone\\n3. Classifiers', ' The sentiment of a sentence should take into account the sentiment of the words in it. So, for example, take the sentence ‚ÄúIt was great to get to study with you!‚Äù. By taking the sentiment of the words ‚Äústudy‚Äù, ‚Äúget‚Äù, and ‚Äúglad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "digital_discourse_notes = [getStudyNotes(\"Digital Discourse\") for _ in range(5)]\n",
        "for note in digital_discourse_notes:\n",
        "  print(note)\n",
        "  print(\"=\"*40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u11YHzV5wJ2f",
        "outputId": "314f598a-b022-49c9-a7b0-6d1c854cda8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " There is a relationship between the word used and the identity of the individual speaker.\n",
            "\n",
            "2. Gender can also change depending on the audience.\n",
            "\n",
            "3. Tone serves as a crucial component for comprehension.\n",
            "\n",
            "4. Slang and jargon differ from culture to culture and language to language\n",
            "\n",
            "5.\n",
            "========================================\n",
            "Doesn't exist in a bubble so we need to pay attention to the framing and the understanding and assumptions around the discourse\n",
            "\n",
            "2.It can be participatory and evolving\n",
            "\n",
            "3.We need to study the social and political dimensions\n",
            "\n",
            "4.Water cooler talk\n",
            "\n",
            "5. A means of both self\n",
            "========================================\n",
            " Digital Discourse includes the many locations of the discussion, such as a blog, a message board, a chatroom, a wiki, a MySpace page, a Facebook page, a YouTube video, etc.\n",
            "A) Different people have different digital practices when it comes to culture\n",
            "\n",
            "2. There are \"four\n",
            "========================================\n",
            " Textual and Visual 2. Readers and Writers 3. Poetic Knowledge and Poetic Practice 4. Technical and Cultural 5. Historical and Scientific 6. Analysis and Interdisciplinarity\n",
            "\n",
            "What is an example of a semantic textual bias in a document?\n",
            "\n",
            "A semantic textual bias in a document is it reads\n",
            "========================================\n",
            " The value of the status quo\n",
            "\n",
            "2. Stance\n",
            "\n",
            "3. The one who doesn't know\n",
            "\n",
            "4. Tools of language\n",
            "\n",
            "5. Summary\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getEssayOutline(subject):\n",
        "  response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=f\"Create an outline for an essay about {subject}:\\n\\nI: Introduction\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=60,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0\n",
        ")\n",
        "  return response[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "u08bJVfkw3e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(getEssayOutline(\"Julius Cesar\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVrdpD2txQlc",
        "outputId": "9db87882-1893-4ba5-f0cf-5301cd9f8bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "II: Julius Cesar\n",
            "\n",
            "III: Family background\n",
            "\n",
            "IIII: Early life\n",
            "\n",
            "IIII: Civil service\n",
            "\n",
            "V: Cesar and Crassus\n",
            "\n",
            "VI: Cesar and Pompey\n",
            "\n",
            "VII: Cesar and the provinces\n",
            "\n",
            "VIII:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getEssayOutline(\"Tourism in Latvia\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMqJRBF9yCa9",
        "outputId": "55146e99-2f98-49e0-ab20-926a1fd6f3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "- Tourism in Latvia.\n",
            "\n",
            "II: What are the main development directions of the tourism business in Latvia?\n",
            "\n",
            "- Growth of the tourism industry.\n",
            "\n",
            "- The tourism industry as one of the most dynamic economic sectors in Latvia.\n",
            "\n",
            "- Importance of the tourism industry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getHorrorStory(topic):\n",
        "  response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=f\"Topic: Breakfast\\nTwo-Sentence Horror Story: He always stops crying when I pour the milk on his cereal. I just have to remember not to let him see his face on the carton.\\n###\\nTopic: {topic}\\nTwo-Sentence Horror Story:\",\n",
        "  temperature=0.5,\n",
        "  max_tokens=60,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.5,\n",
        "  presence_penalty=0.0,\n",
        "  stop=[\"###\"]\n",
        ")\n",
        "  return response[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "wVXXn0Qdy8AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(getHorrorStory(\"snow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUv_ZK8AzLXE",
        "outputId": "ec242bf5-d10e-4f88-dc89-22d5909f3dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I was walking home from work when I realized that I was the only one on the sidewalk. A few minutes later, I saw a snowplow coming down the road. I waved to get its attention, but it just kept on going.\n",
            "I don't know what happened to everyone else.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getHorrorStory(\"Christmas\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCdaEt7XzYjI",
        "outputId": "25c8deba-7d0a-42d4-e1b1-286a57e4de58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The real Santa Claus was too fat to fit down the chimney, so he left a note saying he'd be back next year.\n",
            "Two-Sentence Horror Story: I think Santa Claus is going to kill me.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# once I have a list of some texts to analyzie\n",
        "# i can simply loop over them and clal my getSentiment function for each text/document\n",
        "my_tweets = [\"In October main exports partners were Lithuania, Estonia, Germany and United Kingdom. The main import partners were Lithuania, Russian Federation, Poland and Germany.\"\n",
        ",\"In October 2021 the foreign trade turnover of Latvia amounted to ‚Ç¨ 3.39 billion, which at current prices was 23.5% larger than a year ago.Exports value of goods was ‚¨ÜÔ∏è17.8% higher, but imports value of goods ‚¨ÜÔ∏è28.8% higher. \"]\n",
        "\n",
        "my_tweet_sentiments = [getSentiment(tweet) for tweet in my_tweets]\n",
        "my_tweet_sentiments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaiMIPijzuzr",
        "outputId": "0aa4df47-0c80-41ac-c0bd-d9d237975230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Positive', ' 0']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    }
  ]
}