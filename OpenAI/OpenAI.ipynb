{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenAI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Model Usage in Digital Discourse\n",
        "\n",
        "As of December 2025 (things change fast!): Large Language Models (LLMs) have become a practical methodological tool in Digital Discourse Analysis, not as replacements for theory-driven interpretation, but as instruments that scale and accelerate interpretive labor.\n",
        "\n",
        "Their primary value lies in handling discourse-level phenomena‚Äîsuch as framing, stance, argumentation, and narrative roles‚Äîthat are difficult to capture with traditional rule-based or statistical NLP methods.\n",
        "\n",
        "When used critically and transparently, LLMs enable researchers to annotate large corpora, explore patterns across time and languages, and support comparative analysis that would otherwise be infeasible. At the same time, LLM outputs remain probabilistic, interpretive, and error-prone, making methodological control, validation, and human oversight essential. Properly positioned, LLMs function as advanced analytical assistants within established Digital Discourse frameworks rather than as autonomous authorities on meaning.\n",
        "\n",
        "## 1. Discourse-Level Annotation at Scale\n",
        "\n",
        "- Automatic labeling of discourse features that are contextual and interpretive rather than purely lexical  \n",
        "- Typical annotation tasks:\n",
        "  - Stance detection (support / oppose / neutral)\n",
        "  - Framing detection (economic, moral, security, cultural, etc.)\n",
        "  - Actor roles (government, citizen, expert, institution, scapegoat)\n",
        "  - Narrative positions (victim, threat, hero, authority)\n",
        "  - Politeness, aggression, irony, sarcasm\n",
        "  - Modality and certainty (hedging, obligation, inevitability)\n",
        "- Strengths:\n",
        "  - Handles multi-sentence context\n",
        "  - Performs better than keyword or rule-based systems on indirect language\n",
        "- Methodological notes:\n",
        "  - Use closed and well-defined label sets\n",
        "  - Validate on a human-annotated subset\n",
        "  - Treat outputs as noisy annotations suitable for aggregation\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Argumentation and Rhetorical Structure Extraction\n",
        "\n",
        "- Identification of argument components within texts:\n",
        "  - Claims\n",
        "  - Premises\n",
        "  - Conclusions\n",
        "  - Implicit warrants\n",
        "- Detection of rhetorical strategies:\n",
        "  - Counter-arguments and rebuttals\n",
        "  - Appeals to authority, fear, emotion\n",
        "  - Common logical fallacies (false dilemma, ad hominem, slippery slope)\n",
        "- Suitable text types:\n",
        "  - Political speeches\n",
        "  - Editorials and opinion pieces\n",
        "  - Parliamentary debates\n",
        "  - Long-form social media posts\n",
        "- Limitations:\n",
        "  - Output is interpretive, not ground truth\n",
        "  - Argument structure is inferred, not empirically verified\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Discourse-Sensitive Topic Modeling\n",
        "\n",
        "- Semantic clustering of texts beyond bag-of-words models\n",
        "- Identification of topics that reflect:\n",
        "  - Frames\n",
        "  - Narratives\n",
        "  - Ideological positioning\n",
        "- Typical workflow:\n",
        "  - Chunk documents into manageable segments\n",
        "  - Generate embeddings using an LLM or embedding model\n",
        "  - Cluster segments using statistical methods\n",
        "  - Use an LLM to describe clusters in discourse-aware terms\n",
        "- Useful for:\n",
        "  - Historical corpora\n",
        "  - Parliamentary archives\n",
        "  - Newspaper collections\n",
        "  - Policy documents\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Diachronic Discourse Analysis\n",
        "\n",
        "- Comparison of discourse patterns across time periods\n",
        "- Analysis of shifts in:\n",
        "  - Framing strategies\n",
        "  - Metaphor usage\n",
        "  - Moral language\n",
        "  - Attribution of agency\n",
        "  - Degree of certainty or hedging\n",
        "- Common research questions:\n",
        "  - How discourse changes before and after major events\n",
        "  - How terminology and framing evolve across decades\n",
        "- Practical considerations:\n",
        "  - Avoid feeding entire corpora directly\n",
        "  - Use sampling, summarization, and controlled comparative prompts\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Metaphor and Figurative Language Detection\n",
        "\n",
        "- Identification of metaphorical and figurative expressions\n",
        "- Classification of metaphor types:\n",
        "  - War metaphors\n",
        "  - Disease metaphors\n",
        "  - Journey metaphors\n",
        "  - Market or economic metaphors\n",
        "- Advantages over classical NLP:\n",
        "  - Better handling of implicit and creative metaphors\n",
        "- Caveats:\n",
        "  - Cultural and historical metaphors require domain expertise\n",
        "  - Outputs should be validated against human interpretation\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Discourse Actor and Voice Analysis\n",
        "\n",
        "- Analysis of who is speaking and from which position\n",
        "- Detection of:\n",
        "  - Institutional vs personal voice\n",
        "  - Collective voice (‚Äúwe‚Äù, ‚Äúthe people‚Äù)\n",
        "  - Shifts in perspective within a single text\n",
        "- Identification of agency suppression:\n",
        "  - Passive constructions\n",
        "  - Vague responsibility attribution\n",
        "- Value:\n",
        "  - Difficult to automate reliably without LLM-based interpretation\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Cross-Lingual Discourse Comparison\n",
        "\n",
        "- Comparison of discourse strategies across languages\n",
        "- Identification of:\n",
        "  - Framing differences\n",
        "  - Rhetorical shifts introduced by translation\n",
        "  - Language-specific discourse markers\n",
        "- Best practices:\n",
        "  - Avoid blind reliance on raw machine translation\n",
        "  - Ask the LLM to justify translation and framing choices\n",
        "- Particularly useful for:\n",
        "  - Multilingual political discourse\n",
        "  - International media analysis\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Corpus Exploration and Hypothesis Generation\n",
        "\n",
        "- Use of LLMs as exploratory research assistants\n",
        "- Typical tasks:\n",
        "  - Suggesting discourse categories\n",
        "  - Proposing coding schemes\n",
        "  - Highlighting unexpected patterns\n",
        "- Appropriate use:\n",
        "  - Early-stage exploration\n",
        "  - Supporting qualitative reasoning\n",
        "- Inappropriate use:\n",
        "  - Drawing causal conclusions\n",
        "  - Replacing formal statistical analysis\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Teaching and Pedagogical Applications\n",
        "\n",
        "- Classroom uses:\n",
        "  - Annotating sample texts\n",
        "  - Comparing human and LLM-based analyses\n",
        "  - Prompt engineering as a methodological exercise\n",
        "  - Critical evaluation of bias and hallucination\n",
        "- Educational value:\n",
        "  - Enhances students‚Äô critical discourse awareness\n",
        "  - Makes methodological limitations explicit rather than hidden\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Known Limitations of LLMs in Discourse Analysis (as of 2025)\n",
        "\n",
        "- LLMs should not be relied upon to:\n",
        "  - Produce accurate frequency counts without verification\n",
        "  - Replace close reading and contextual analysis\n",
        "  - Infer author intent reliably\n",
        "  - Deliver unbiased or neutral interpretations\n",
        "- Recommended stance:\n",
        "  - Treat LLMs as interpretive tools\n",
        "  - Maintain human oversight and theoretical grounding"
      ],
      "metadata": {
        "id": "n4Ka-707iBXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing OpenAI library\n",
        "\n",
        "Principles will be very similar to other AI libraries (Mistral, etc)\n",
        "\n",
        "Open AI library is nactually currently included in Google Colab but not the latest version.\n",
        "\n",
        "\n",
        "I would imagine Google would want to include some of their products(that is libraries to access them).\n",
        "Those are Bard and Gemini (as of December 2023).\n",
        "\n",
        "* Google did most of the original reasearch on Large Language Models in the last 15 years - but has not capitalized on it\n",
        "* Instead companies such as OpenAI have overtaken Google in the mind-share and usability\n",
        "* It is a classical case of Innovators Dilemma - Google was making too much money on their Search and they were afraid to hurt their product, now they have little choice\n",
        "(lots of analogies elsewhere, for example BMW early electrical car looked very different from standard BMW because they were afraid of hurting their main product, now BMW realized that they have to catch up to companies such as Tesle, etc)\n",
        "\n",
        "OpenAI is not immune, there are companies, startups such as Mistral from France, Europe that are very promising and may overtake OpenAI.\n",
        "Competition in this field is high - and OpenAI could use some competition, because OpenAI is not actually open...\n",
        "\n",
        "So pragmatically for now we will be using OpenAI to do our LLM tasks."
      ],
      "metadata": {
        "id": "cPPDxkbtuoRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai # in 2024 the library was not present but now is\n",
        "print(f\"Open AI version {openai.__version__}\")\n",
        "# compare to version at pypi.org\n",
        "# https://pypi.org/project/openai/"
      ],
      "metadata": {
        "id": "Aqx-Uk4yn7Dn",
        "outputId": "8b845003-731c-4ea0-b410-3ca0d8e1e78f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open AI version 2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so not needed anymore but you can use it to install later versions\n",
        "# !pip install --upgrade --force-reinstall openai\n",
        "# THEN RESTART RUNTIME!"
      ],
      "metadata": {
        "id": "KZnYIGn7bsV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "snFB8EfCbE8y"
      },
      "outputs": [],
      "source": [
        "import os # standard Python library for os related tasks\n",
        "# import openai # so this is openai library we just installed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting OpenAI API key\n",
        "\n",
        "It used to be that you could get OpenAI key for free at least for a month and you could get some credits. Maybe you still get one month or $10 free credits.\n",
        "\n",
        "The site for OpenAI Develope page is: https://platform.openai.com/docs/overview\n",
        "\n",
        "To get key go here: https://platform.openai.com/api-keys\n",
        "\n",
        "KEY IDEA: Never show API keys in your code or notebooks!!!\n"
      ],
      "metadata": {
        "id": "z-XBAO89BCMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata # API to access user secrets\n",
        "\n",
        "# for Google Colab users the best way to save Secrets is to use Secrets storage provided by Google Colab\n",
        "# let's get secret key by name from Secrets storage: OPENAI_DH\n",
        "\n",
        "secret = userdata.get(\"OPENAI_DH\")\n",
        "# how long is secret?\n",
        "print(f\"Secret is {len(secret)} characters long\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXnE95iaaVK5",
        "outputId": "97b44382-dea3-461a-f494-43c6f8a84d40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secret is 164 characters long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# alternative to give secret by hand\n",
        "if not secret:\n",
        "  from getpass import getpass # standard library into Python\n",
        "  secret = getpass('Enter the secret API key for OpenAI value: ') # so very similar to input function but with stars...\n",
        "else:\n",
        "  print(f\"I already have secret key\")\n",
        "\n",
        "# the 3rd option would be to load the secret key from some configuration or .env file that is also fine AS LONG as that file is not publicly visible anywhere :0"
      ],
      "metadata": {
        "id": "Zf5SaV5Wu7JT",
        "outputId": "52070610-9833-462f-d4b4-01bf15839bde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I already have secret key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# openai.api_key = \"use_your_own_key!\"  #never share your private API keys with the world! read them from enviroment or private text file\n",
        "# or using getpass and copy pasting (like I did - not very convenient but good for one time use)\n",
        "# alternatives, store API keys on your Google Drive - personally I do not recommend\n",
        "# there is also something called Google Secrets, again I personally do not trust it, but your mileage may vary\n",
        "openai.api_key = secret  #never share your private API keys with the world! read them from enviroment or private text file"
      ],
      "metadata": {
        "id": "DVAjoD-bb4Vf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing the API\n",
        "\n",
        "https://github.com/openai/openai-python - documents the changes\n",
        "\n",
        "Since this is cutting edge research, API changes quite often.\n",
        "\n",
        "Expect things to stabilize in a few years."
      ],
      "metadata": {
        "id": "NUcrvN9ivPWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    api_key=secret, # instead of accessing my api key from my os, i use the key i pasted in to secret\n",
        "    # again secret is just a string but do not write it directly here!!\n",
        ")"
      ],
      "metadata": {
        "id": "aFmfUYbFvmWb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    # so we pass one or messages to OpenAI endpoint, in this just\n",
        "    # effectively messages are just a list of dictionaries\n",
        "    # here a single dictionary representing a single message\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Say this is a test but translate into Latvian\", # so this is your prompt\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-5-nano-2025-08-07\", # this is among the cheapest of the models similar to the free version on ChatGPT\n",
        "    # there are more options but we will stick with basics\n",
        ")\n",
        "# so with this example we made a call to OpenAI using the API key"
      ],
      "metadata": {
        "id": "HkelQakVvedo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can get the json response of everything the model provided\n",
        "# chat_completion.json()\n",
        "chat_completion.model_dump_json() # so this is the new approach as of 2025"
      ],
      "metadata": {
        "id": "CP9VHpHevzmc",
        "outputId": "f1e0b554-35a3-4c74-e7fd-e4be2963b1fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"id\":\"chatcmpl-Co9i2WTJZUohB7EIhok8PyX6Yat47\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"Pateik, ka tas ir tests.\",\"refusal\":null,\"role\":\"assistant\",\"annotations\":[],\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1766069758,\"model\":\"gpt-5-nano-2025-08-07\",\"object\":\"chat.completion\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":2770,\"prompt_tokens\":16,\"total_tokens\":2786,\"completion_tokens_details\":{\"accepted_prediction_tokens\":0,\"audio_tokens\":0,\"reasoning_tokens\":2752,\"rejected_prediction_tokens\":0},\"prompt_tokens_details\":{\"audio_tokens\":0,\"cached_tokens\":0}}}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now to get just the answer in text form # so you can see that Latvian is not the greatest here\n",
        "chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "1L3M1pPewD5Q",
        "outputId": "87876f91-6ce8-43c3-c167-1a157d416507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pateik, ka tas ir tests.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chat_completion.choices) # how many choices do we have?"
      ],
      "metadata": {
        "id": "am4dU3p9wvqr",
        "outputId": "741e2396-2353-4cbd-8418-b030621140d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jd-lg20kyCZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automating Sentiment Analysis\n",
        "\n",
        "To avoid writing all the boileplate code by hand, instead I will write a function that combines all the required code and I can call the function instead, whenever I need sentiment analysis."
      ],
      "metadata": {
        "id": "J3BljXVmHLYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# so we can predefine our own commonly used settings such as partial prompts, models, how many input symbols to use, etc.\n",
        "def getSentiment(prompt, client=client, sentiments=(\"positive\",\"neutral\",\"negative\"),\n",
        "                #  model=\"gpt-3.5-turbo\", # from 2024\n",
        "                 model=\"gpt-5-nano-2025-08-07\", # cheapest from 2025\n",
        "                 max_prompt=200 # you can control how many tokens you get in input\n",
        "                 ):\n",
        "    sentiment_text = \",\".join(sentiments)  # I add all the sentiments in a string separated by comma\n",
        "    prompt=f\"Social media post: \\\"{prompt[:max_prompt]}\\\"Sentiment ({sentiment_text}):\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "uqSeqhEcyFY-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"I really like bread and circuses\")\n",
        "# depending on temperature setting we might get different answers"
      ],
      "metadata": {
        "id": "gwvz0Tbvyn2z",
        "outputId": "ff390711-4f54-4155-90a5-a8cafb85306b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Positive\\n\\nReason: The post states that the author ‚Äúreally likes‚Äù bread and circuses, which expresses a positive sentiment toward the concept. (Context or sarcasm could alter this in some cases.)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Where do I begin? This is a brand new 4K scan from the original negative of the movie with an added HDR10 & Dolby Vision HDR grading, which looks fantastic!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yrZMnvmhnx1W",
        "outputId": "587c2cfa-b321-4dd9-bf6c-cffaa3308f1e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Man patik alus\") # I like beer in Latvian"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Esn2RaidoN_n",
        "outputId": "c5d172b1-7450-4757-c88e-5a76cd98d997"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Man nepatƒ´k slidenas ielas\") #I do not like slippery streets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0ZdW9uicoWXh",
        "outputId": "d3bf25f2-9360-4d61-edb3-02890aa7a4a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using this custom function you can pass in your own sentiments - for example in Latvian\n",
        "getSentiment(\"Man nepatƒ´k slidenas ielas\", sentiments=[\"pozitƒ´vs\", \"neitrƒÅls\", \"negatƒ´vs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vj0lBNvfpDk4",
        "outputId": "22c5be4d-eb0b-40fc-948f-007de11d9c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negatƒ´vs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Man patik alus\", sentiments=[\"pozitƒ´vs\", \"neitrƒÅls\", \"negatƒ´vs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I1uBEGwQpShR",
        "outputId": "d702565e-87f0-4df6-9272-0d49eb5c36c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pozitƒ´vs. (Izsaka patiku pret alu.)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt creation\n",
        "\n",
        "So basic  idea is we provide a sort of answer key when we need sentiment analysis. We ended our prompt with possible answers and LLM gave us one of them."
      ],
      "metadata": {
        "id": "YRWwxTzvIw2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# so this is our own ChatGPT basically, except we can adjust more parameters\n",
        "def getAnswer(prompt, client=client,\n",
        "              # model=\"gpt-3.5-turbo\",\n",
        "              model=\"gpt-5-nano-2025-08-07\",\n",
        "              max_prompt=500):\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt[:max_prompt],\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "FWvsAxxhJE-J"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getAnswer(\"What is the capital of Latvia?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HPIGW71mJaUy",
        "outputId": "850cfa90-c9f5-4f18-8211-13f0232693e8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Riga.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# of course LLMs have a infamous tendendcy to hallucinate especially when the model does not have the full answer.\n",
        "# One possible test to evaluate is to ask a question where the answer is between ranges of some information it already knows\n",
        "getAnswer(\"Who was president of Latvia in 1926?\") # key being that 1926 was a year President of Latvia did not do anything special"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vmMYuABQJjZQ",
        "outputId": "cce6a9c1-174f-4920-807b-2e0ab9e500ae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'JƒÅnis ƒåakste. He served as President of Latvia from 1922 until his death in 1927, so in 1926 the president was ƒåakste.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getAnswer(\"Who was president of Latvia in 1926? Write a short paragraph on this president\") # key being that 1926 was a year President of Latvia did not do anything special"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "-IPEVFv0KB8q",
        "outputId": "8bf119aa-bec0-4d54-9482-5989e25c9482"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'JƒÅnis ƒåakste.\\n\\nShort paragraph:\\nJƒÅnis ƒåakste was the first President of Latvia, serving from 1922 until his death in 1927. A Latvian lawyer and statesman, he played a pivotal role in shaping Latvia‚Äôs early independence and constitutional framework, guiding the country through its formative years with a focus on parliamentary democracy and political stability. He remained in office through 1926 and died in 1927, leaving a legacy of constitutional integrity and steady leadership during Latvia‚Äôs interwar period.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### getAnswer as analogous to ChatGPT interface\n",
        "\n",
        "So now if you had say 100 documents to analyse you could write a loop and pass this function 100 times."
      ],
      "metadata": {
        "id": "rS-2_WCJKSes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# so there is this idea in prompting LLMs called shots, zero-shot (no examples)\n",
        "# one shot - single example\n",
        "# few shot - some examples\n",
        "def getMovieEmoji(movie_title,client=client,\n",
        "                  # model=\"gpt-3.5-turbo\"\n",
        "                  model=\"gpt-5-nano-2025-08-07\"\n",
        "                  ):\n",
        "    prompt=f\"\"\"Back to Future: üë®üë¥üöóüïí\n",
        "    Batman: ü§µü¶á\n",
        "    Transformers: üöóü§ñ\n",
        "    Wonder Woman: üë∏üèªüë∏üèºüë∏üèΩüë∏üèæüë∏üèø\n",
        "    Winnie the Pooh: üêªüêºüêª\n",
        "    The Godfather: üë®üë©üëßüïµüèª‚Äç‚ôÇÔ∏èüë≤üí•\n",
        "    Game of Thrones: üèπüó°üó°üèπ\n",
        "    {movie_title}: \"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "SWXEr_F9qbtH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iN-UHwVgK-1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(getMovieEmoji(\"The Bourne Conspiracy\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XGO_WoNq6eM",
        "outputId": "a00b8db7-98a1-4d5d-a97e-e582d77900e9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Bourne Conspiracy: üïµÔ∏è‚Äç‚ôÇÔ∏èüß†üí•üöóüí®üèÉ‚Äç‚ôÇÔ∏èüîé\n",
            "\n",
            "Reason: spy/amnesia themes (detective), memory issues (brain), action/explosions, car chases, and pursuit/investigation. Want another variation (more focused on amnesia or CIA)? I can adjust.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so let's try with 3.5\n",
        "print(getMovieEmoji(\"The Bourne Conspiracy\", model=\"gpt-3.5-turbo\")) # much quicker, but might not be as relevant"
      ],
      "metadata": {
        "id": "DuCfyWGrAKEG",
        "outputId": "3428b8a6-0aae-445b-a457-8da289d0bb64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üïµÔ∏èüïµÔ∏è‚Äç‚ôÇÔ∏èüî´üí•üî™\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getMovieEmoji(\"Top Gun: Maverick\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqqJRsogLgvy",
        "outputId": "58641588-e1e0-4e17-e4f1-060324ab120e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nice puzzle. Here are a few emoji-clue options for Top Gun: Maverick. Pick the vibe you prefer (jet/flight emphasis, Navy pilot, speed, etc.):\n",
            "\n",
            "- Option 1: ‚úàÔ∏èüë®‚Äç‚úàÔ∏èüî•üèÅ\n",
            "  - Jet, pilot, intensity, finish/speed vibe (Maverick‚Äôs famous need for speed).\n",
            "\n",
            "- Option 2: üõ©Ô∏èüë®‚Äç‚úàÔ∏èüåäüèüÔ∏è\n",
            "  - Fighter jet, Navy pilot, carrier/sea setting, action-packed.\n",
            "\n",
            "- Option 3: ‚úàÔ∏èüßë‚Äç‚úàÔ∏èüí•üí®\n",
            "  - Jet, pilot, explosion/high-adrenaline moments, speed lines.\n",
            "\n",
            "- Option 4: üõ´üë®‚Äç‚úàÔ∏èü™ñüèÅ\n",
            "  - Takeoff, pilot, military deco, finish line (competition/flight goal).\n",
            "\n",
            "If you want a single, compact set: ‚úàÔ∏èüë®‚Äç‚úàÔ∏èüî•üèÅ captures the core idea of Top Gun and Maverick nicely.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getMovieEmoji(\"Frozen 2\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaIIowiALnnO",
        "outputId": "3642ac34-3fcf-47ca-eb3e-5094c07dcdac"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nice idea. Here are some Frozen 2 emoji options you can use, focusing on the two sisters, the forest journey, and the elemental spirits.\n",
            "\n",
            "- Basic: ‚ùÑÔ∏èüë≠üå≤ü™Ñ\n",
            "- With the four elements (fire, water, wind, earth) hinted: ‚ùÑÔ∏èüë≠üî•üíßüí®üå≤ü™®\n",
            "- Including the journey/kingdom vibe: ‚ùÑÔ∏èüë≠üèîÔ∏èüå≤ü™Ñüíß\n",
            "- Nokk and forest spirits vibe: ‚ùÑÔ∏èüë≠üåäüêéüå¨Ô∏èü™Ñüå≤\n",
            "\n",
            "Want me to tailor it to a specific aspect (Elsa/Anna, the forest, the spirits, or the songs) or make a shorter/longer version?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key takeaway - answers are not deterministic\n",
        "\n",
        "Notice how multiple calls can return different answers, it is due to the temperature setting that slightly changes the answers.\n",
        "\n",
        "Model stays the same but there is an element of slight randomness present.\n",
        "\n",
        "So when you see some amazing LLM examples in action you have to ask how many tries did it take to generate them. Case in point recent Google Gemini Demo presentation."
      ],
      "metadata": {
        "id": "HKSOVVzELvon"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEDcNi-jLudY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.esrb.org/\n",
        "def getGameRating(movie_text,client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"\"\"\"Provide an ESRB rating for the following text:\n",
        "    {movie_text}\n",
        "    ESRB rating:\"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_tokens=60,\n",
        "        temperature=0.7 # temperature ranges from 0 to 2\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "7pIUj35jsAdM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getGameRating(\"The game opens with a screen in a dark and stormy night. Five gamblers sit around a fire in a dark forest.\")\n",
        "# so one of the issues with LLMs is that you need to provide enought context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-NDcBK7jNJZW",
        "outputId": "5230330f-3b1c-4329-fc77-20849e740893"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Teen - Mild Violence, Simulated Gambling'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getMovieRating(movie_text,client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"\"\"\"Provide an MPA rating to the movie based on following description:\n",
        "    {movie_text}\n",
        "    MPA rating:\"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_tokens=60,\n",
        "        temperature=0.7 # temperature ranges from 0 to 1\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "GbR698W1ub-_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getMovieRating(\"They say all happy families are alike but all unhappy families are different in their own way\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IaObBgequjrG",
        "outputId": "d3c8a0d0-7c98-4927-dd07-71e43a9d4378"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PG-13'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getStudyNotes(subject, client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"What are some key points I should know when studying {subject} Provide at least five key points\\n\\n1.\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        # using default for now, you can play around with some of the parameters\n",
        "        # temperature=1,\n",
        "        # max_tokens=64,\n",
        "        # top_p=1.0,\n",
        "        # frequency_penalty=0.0,\n",
        "        # presence_penalty=0.0\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "Ti9Vdw62u2AY"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(getStudyNotes(\"Riga\")) # so this is from 3.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Aqk-WJTvZh5",
        "outputId": "6605ab77-2fc5-450b-d8ad-65f39326c9bd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riga is the capital and largest city of Latvia, located on the Baltic Sea coast.\n",
            "2. The city's historic center, known as Old Riga, is a UNESCO World Heritage Site and is home to well-preserved medieval architecture including cathedrals, churches, and charming cobblestone streets.\n",
            "3. Riga is known for its vibrant cultural scene, with numerous museums, art galleries, theaters, and music venues showcasing the city's rich history and contemporary creativity.\n",
            "4. The city is a popular destination for tourists, offering a wide range of attractions such as the Riga Central Market, the Latvian National Opera, and the Art Nouveau district filled with stunning architectural masterpieces.\n",
            "5. Riga is also a hub for business and commerce in the Baltic region, with a growing economy and a diverse range of industries including finance, technology, and logistics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's try it with latest and greatest 5.2 - came out in December 2025\n",
        "print(getStudyNotes(\"Riga\", model=\"gpt-5.2\"))# this is the full version not NANO"
      ],
      "metadata": {
        "id": "DxLncRpfcVMK",
        "outputId": "e821783f-f06e-4f81-e62c-764618c24113",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **Geography & setting**  \n",
            "   - Riga is the **capital of Latvia**, located on the **Daugava River** near the **Gulf of Riga (Baltic Sea)**. Its position made it a major **trade and port city**.\n",
            "\n",
            "2. **Historical development**  \n",
            "   - Founded in **1201**, Riga grew as an important city in the **Hanseatic League**, connecting Baltic trade routes with Northern Europe.\n",
            "\n",
            "3. **Cultural and political influences**  \n",
            "   - Riga‚Äôs history reflects shifting rule and influence from **Germanic elites**, **Sweden**, the **Russian Empire**, and later the **Soviet Union**, shaping its language politics, institutions, and demographics.\n",
            "\n",
            "4. **Architecture & UNESCO significance**  \n",
            "   - Riga‚Äôs historic center is a **UNESCO World Heritage Site**, known for its **Art Nouveau (Jugendstil)** architecture‚Äîone of the largest and best-preserved collections in Europe‚Äîalongside medieval Old Town buildings.\n",
            "\n",
            "5. **Modern Latvia: economy and society**  \n",
            "   - Today Riga is Latvia‚Äôs **largest city and economic hub**, central to finance, services, transport, and tourism, and it plays a major role in Latvia‚Äôs identity after independence (restored in **1991**).\n",
            "\n",
            "6. **Demographics & languages (useful for social studies)**  \n",
            "   - Riga is ethnically mixed compared with rural Latvia; **Latvian is the official language**, but **Russian is widely spoken**, which matters for understanding politics, education, and media.\n",
            "\n",
            "If you tell me whether you‚Äôre studying Riga for **history, geography, architecture, or politics**, I can tailor the key points and include dates, key events, and notable sites.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i am using print because this answer includes newlines\n",
        "study_notes_on_dd = getStudyNotes(\"Digital Discourse\")\n",
        "print(study_notes_on_dd)\n",
        "# note how 1. is missing from the answer?\n",
        "# why?\n",
        "# because I already provide 1. at the end of my prompt\n",
        "# remember LLMs are just word prediction machines (with some useful side effects)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FERP6tSvO9py",
        "outputId": "ad43f827-1643-4dc3-ddbf-199306288aad"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digital discourse refers to communication practices that take place online, such as social media interactions, email exchanges, and online forums.\n",
            "\n",
            "2. The tone and language used in digital discourse can vary greatly depending on the platform and the relationship between the communicators. It is important to understand the context in which communication is taking place in order to interpret it correctly.\n",
            "\n",
            "3. Digital discourse can be influenced by factors such as anonymity, immediacy, and the ability to communicate with a large audience. These factors can impact the way people communicate and the content of their messages.\n",
            "\n",
            "4. Understanding digital discourse can help individuals analyze and critically evaluate online communication, including identifying bias, misinformation, and manipulation tactics used in digital spaces.\n",
            "\n",
            "5. Studying digital discourse can also help individuals improve their own online communication skills, such as writing clear and effective messages, engaging with others in a respectful manner, and utilizing digital tools and platforms effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now i can save my notes to text file\n",
        "with open(\"study_notes.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(study_notes_on_dd)"
      ],
      "metadata": {
        "id": "lXFUi6UFPKOc"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do the same with 5.2\n",
        "# let's generate nots on 5.2\n",
        "study_notes_on_dd_52 = getStudyNotes(\"Digital Discourse\", model=\"gpt-5.2\")\n",
        "print(study_notes_on_dd_52)"
      ],
      "metadata": {
        "id": "hUi38vuvcld0",
        "outputId": "9c9a2517-3f8e-42d8-cec2-0d5484744ba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **Digital discourse is shaped by the platform.**  \n",
            "   Each platform‚Äôs features (character limits, threading, ‚Äúlikes,‚Äù hashtags, algorithmic feeds, moderation tools) influence what people say, how they say it, and how messages spread.\n",
            "\n",
            "2. **Meaning is often multimodal, not just textual.**  \n",
            "   Digital communication commonly combines text with images, GIFs, emojis, audio, video, formatting, and links‚Äîso you study how these modes work together to create meaning.\n",
            "\n",
            "3. **Context and audience are fluid (‚Äúcontext collapse‚Äù).**  \n",
            "   Online messages can reach multiple audiences at once (friends, employers, strangers), and can be reshared beyond the original setting‚Äîchanging interpretation and raising risks around privacy and reputation.\n",
            "\n",
            "4. **Identity and stance are actively performed.**  \n",
            "   Users construct identities through usernames, profile elements, language style, memes, and interaction patterns. You‚Äôll often analyze how people signal tone, credibility, expertise, humor, or alignment with groups.\n",
            "\n",
            "5. **Interaction is highly networked and participatory.**  \n",
            "   Replies, quote-posts, tagging, duets/stitches, and comment threads make discourse collaborative. Studying digital discourse includes how conversations form, escalate, derail, or build communities.\n",
            "\n",
            "6. **Power, norms, and moderation matter.**  \n",
            "   Platform rules, community guidelines, and moderation practices shape whose voices are amplified or silenced. You‚Äôll likely examine harassment, gatekeeping, misinformation, and the politics of ‚Äúvisibility.‚Äù\n",
            "\n",
            "7. **Digital discourse leaves traces (ethics and data).**  \n",
            "   Online communication can be persistent, searchable, and analyzable. Studying it involves research ethics: consent, anonymization, quoting posts, private vs. public spaces, and data security.\n",
            "\n",
            "If you tell me what course level you‚Äôre at (high school/undergrad/grad) and which platforms you‚Äôre focusing on, I can tailor the key points and give example analyses.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion on use of LLMs in Digital Discourse\n",
        "\n",
        "So LLMs can aid in discourse analysis with what:\n",
        "\n",
        "* Automated Text Analysis\n",
        "* Sentiment Analysis\n",
        "* Topic Modeling\n",
        "* Translation\n",
        "* Context Analysis\n",
        "* Cross-cultural Analyss\n",
        "* Summarization\n",
        "* Bias detection"
      ],
      "metadata": {
        "id": "f1HvTyjRPvp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notes = [getStudyNotes(\"Sentiment Analysis\") for _ in range(5)]  # i ran the same query 5 times, so text completion will be different each tie\n",
        "print(notes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKmTUYYcvkuR",
        "outputId": "0afb34d8-8cd1-4607-aea3-87e00b7e4afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Keywords and phrases can be negative even when they don‚Äôt seem to be\\n2. Popular sentiment analysis is not always the right analysis\\n3. Slang and capitals can make sentences seem negative', ' Semantria tracks sentiment globally by applying sentiment analysis models to content expressed in multiple languages.\\n2. Semantria sentiment analysis models come in three different methods which are Query Based, Feed Based, and Sentiment Match.\\n3. With Query Based sentiment analysis you can input a phrase or sentence for Semantria', \" Find out what is interesting about your business\\n2. Focus on your buying process\\n\\n3. Reduce capital inventory\\n4. Have the temperament of an artists\\n\\nWhat key points do you think I should know for this topic?\\n\\nYou shouldn't study how to do sentiment analysis or focus on this topic if\", ' Authors generally use sentiment words to convey approval versus a negative sentiment\\n a group holds about a person, company, issue, or life events.\\n2. Automated sentiment analysis is a technique for analyzing texts to make scientifically-based assessments on whether a text is of a broadly positive or negative tone\\n3. Classifiers', ' The sentiment of a sentence should take into account the sentiment of the words in it. So, for example, take the sentence ‚ÄúIt was great to get to study with you!‚Äù. By taking the sentiment of the words ‚Äústudy‚Äù, ‚Äúget‚Äù, and ‚Äúglad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "digital_discourse_notes = [getStudyNotes(\"Digital Discourse\") for _ in range(5)]\n",
        "for note in digital_discourse_notes:\n",
        "  print(note)\n",
        "  print(\"=\"*40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u11YHzV5wJ2f",
        "outputId": "314f598a-b022-49c9-a7b0-6d1c854cda8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " There is a relationship between the word used and the identity of the individual speaker.\n",
            "\n",
            "2. Gender can also change depending on the audience.\n",
            "\n",
            "3. Tone serves as a crucial component for comprehension.\n",
            "\n",
            "4. Slang and jargon differ from culture to culture and language to language\n",
            "\n",
            "5.\n",
            "========================================\n",
            "Doesn't exist in a bubble so we need to pay attention to the framing and the understanding and assumptions around the discourse\n",
            "\n",
            "2.It can be participatory and evolving\n",
            "\n",
            "3.We need to study the social and political dimensions\n",
            "\n",
            "4.Water cooler talk\n",
            "\n",
            "5. A means of both self\n",
            "========================================\n",
            " Digital Discourse includes the many locations of the discussion, such as a blog, a message board, a chatroom, a wiki, a MySpace page, a Facebook page, a YouTube video, etc.\n",
            "A) Different people have different digital practices when it comes to culture\n",
            "\n",
            "2. There are \"four\n",
            "========================================\n",
            " Textual and Visual 2. Readers and Writers 3. Poetic Knowledge and Poetic Practice 4. Technical and Cultural 5. Historical and Scientific 6. Analysis and Interdisciplinarity\n",
            "\n",
            "What is an example of a semantic textual bias in a document?\n",
            "\n",
            "A semantic textual bias in a document is it reads\n",
            "========================================\n",
            " The value of the status quo\n",
            "\n",
            "2. Stance\n",
            "\n",
            "3. The one who doesn't know\n",
            "\n",
            "4. Tools of language\n",
            "\n",
            "5. Summary\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getEssayOutline(subject):\n",
        "  response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=f\"Create an outline for an essay about {subject}:\\n\\nI: Introduction\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=60,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0\n",
        ")\n",
        "  return response[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "u08bJVfkw3e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(getEssayOutline(\"Julius Cesar\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVrdpD2txQlc",
        "outputId": "9db87882-1893-4ba5-f0cf-5301cd9f8bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "II: Julius Cesar\n",
            "\n",
            "III: Family background\n",
            "\n",
            "IIII: Early life\n",
            "\n",
            "IIII: Civil service\n",
            "\n",
            "V: Cesar and Crassus\n",
            "\n",
            "VI: Cesar and Pompey\n",
            "\n",
            "VII: Cesar and the provinces\n",
            "\n",
            "VIII:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getEssayOutline(\"Tourism in Latvia\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMqJRBF9yCa9",
        "outputId": "55146e99-2f98-49e0-ab20-926a1fd6f3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "- Tourism in Latvia.\n",
            "\n",
            "II: What are the main development directions of the tourism business in Latvia?\n",
            "\n",
            "- Growth of the tourism industry.\n",
            "\n",
            "- The tourism industry as one of the most dynamic economic sectors in Latvia.\n",
            "\n",
            "- Importance of the tourism industry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getHorrorStory(topic):\n",
        "  response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=f\"Topic: Breakfast\\nTwo-Sentence Horror Story: He always stops crying when I pour the milk on his cereal. I just have to remember not to let him see his face on the carton.\\n###\\nTopic: {topic}\\nTwo-Sentence Horror Story:\",\n",
        "  temperature=0.5,\n",
        "  max_tokens=60,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.5,\n",
        "  presence_penalty=0.0,\n",
        "  stop=[\"###\"]\n",
        ")\n",
        "  return response[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "wVXXn0Qdy8AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(getHorrorStory(\"snow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUv_ZK8AzLXE",
        "outputId": "ec242bf5-d10e-4f88-dc89-22d5909f3dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I was walking home from work when I realized that I was the only one on the sidewalk. A few minutes later, I saw a snowplow coming down the road. I waved to get its attention, but it just kept on going.\n",
            "I don't know what happened to everyone else.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getHorrorStory(\"Christmas\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCdaEt7XzYjI",
        "outputId": "25c8deba-7d0a-42d4-e1b1-286a57e4de58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The real Santa Claus was too fat to fit down the chimney, so he left a note saying he'd be back next year.\n",
            "Two-Sentence Horror Story: I think Santa Claus is going to kill me.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# once I have a list of some texts to analyzie\n",
        "# i can simply loop over them and clal my getSentiment function for each text/document\n",
        "my_tweets = [\"In October main exports partners were Lithuania, Estonia, Germany and United Kingdom. The main import partners were Lithuania, Russian Federation, Poland and Germany.\"\n",
        ",\"In October 2021 the foreign trade turnover of Latvia amounted to ‚Ç¨ 3.39 billion, which at current prices was 23.5% larger than a year ago.Exports value of goods was ‚¨ÜÔ∏è17.8% higher, but imports value of goods ‚¨ÜÔ∏è28.8% higher. \"]\n",
        "\n",
        "my_tweet_sentiments = [getSentiment(tweet) for tweet in my_tweets]\n",
        "my_tweet_sentiments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaiMIPijzuzr",
        "outputId": "0aa4df47-0c80-41ac-c0bd-d9d237975230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Positive', ' 0']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternatives to OpenAI API\n",
        "\n",
        "These days you have many many alternative LLM providers, such as Gemini from Google, Claude from Anthropic, Mistral, DeepSeek and many more\n",
        "\n",
        "You could apply for API keys at individual ones, but my recommendation is to use an aggregrator.\n",
        "\n",
        "I personally use https://openrouter.ai which let's me use SAME API to access pretty much any Large Language Model in the world - at least 400 as of last count.\n",
        "\n",
        "Downside you pay about 5% premium for this convenience.\n",
        "\n",
        "Sometimes this is actually unavoidable if you want to connect to some APIs as some providers are very hard to get a paid account and set it up, even Google makes things too difficult at times.\n",
        "\n",
        "Another advantage of Openrouter.ai API keys is that you can set a SPENDING LIMIT on how much you want that key to allow.\n",
        "\n",
        "This is crucial if you are worried of letting some script run wild and rack up huge bills. :)\n",
        "\n"
      ],
      "metadata": {
        "id": "W-pWvj1qdT7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Premade Jupyter Notebook for Batch processing\n",
        "\n",
        "If you want to avoid coding your own notebook, you can use\n",
        "\n",
        "While working at various projects at National Library of Latvia, I've had colleagues ask for batch processing notebook:\n",
        "\n",
        "You are also free to use it - just use your own openrouter.ai key:\n",
        "\n",
        "https://colab.research.google.com/github/LNB-DH/PublicTools/blob/main/notebooks/llm/batch_processing.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "dF63GSJieM12"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4jyYXiueJlc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}