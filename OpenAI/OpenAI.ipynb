{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenAI.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing OpenAI library\n",
        "\n",
        "Principles will be very similar to other AI libraries (Mistral, etc)\n",
        "\n",
        "Open AI library is not currently included in Google Colab.\n",
        "I would imagine Google would want to include some of their products(that is libraries to access them).\n",
        "Those are Bard and Gemini (as of December 2023).\n",
        "\n",
        "* Google did most of the original reasearch on Large Language Models in the last 15 years - but has not capitalized on it\n",
        "* Instead companies such as OpenAI have overtaken Google in the mind-share and usability\n",
        "* It is a classical case of Innovators Dilemma - Google was making too much money on their Search and they were afraid to hurt their product, now they have little choice\n",
        "(lots of analogies elsewhere, for example BMW early electrical car looked very different from standard BMW because they were afraid of hurting their main product, now BMW realized that they have to catch up to companies such as Tesle, etc)\n",
        "\n",
        "OpenAI is not immune, there are companies, startups such as Mistral from France, Europe that are very promising and may overtake OpenAI.\n",
        "Competition in this field is high - and OpenAI could use some competition, because OpenAI is not actually open...\n",
        "\n",
        "So pragmatically for now we will be using OpenAI to do our LLM tasks."
      ],
      "metadata": {
        "id": "cPPDxkbtuoRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZnYIGn7bsV8",
        "outputId": "ba635cb0-35d1-4246-a55e-1e45b0381b95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "snFB8EfCbE8y"
      },
      "outputs": [],
      "source": [
        "import os # standard Python library for os related tasks\n",
        "import openai # so this is openai library we just installed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting OpenAI API key\n",
        "\n",
        "It used to be that you could get OpenAI key for free at least for a month and you could get some credits. Maybe you still get one month or $10 free credits."
      ],
      "metadata": {
        "id": "z-XBAO89BCMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata # API to access user secrets\n",
        "\n",
        "# for Google Colab users the best way to save Secrets is to use Secrets storage provided by Google Colab\n",
        "# let's get secret key by name from Secrets storage: OPENAI_DH\n",
        "\n",
        "secret = userdata.get(\"OPENAI_DH\")\n",
        "# how long is secret?\n",
        "print(f\"Secret is {len(secret)} characters long\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXnE95iaaVK5",
        "outputId": "c94f09d1-087c-45ef-8564-f0e8818c3b24"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secret is 164 characters long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# alternative to give secret by hand\n",
        "if not secret:\n",
        "  from getpass import getpass # standard library into Python\n",
        "  secret = getpass('Enter the secret API key for OpenAI value: ') # so very similar to input function but with stars...\n",
        "else:\n",
        "  print(f\"I already have secret key\")"
      ],
      "metadata": {
        "id": "Zf5SaV5Wu7JT",
        "outputId": "3f9e71dc-f7a3-4062-aa56-4c68cbc3d7ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I already have secret key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# openai.api_key = \"use_your_own_key!\"  #never share your private API keys with the world! read them from enviroment or private text file\n",
        "# or using getpass and copy pasting (like I did - not very convenient but good for one time use)\n",
        "# alternatives, store API keys on your Google Drive - personally I do not recommend\n",
        "# there is also something called Google Secrets, again I personally do not trust it, but your mileage may vary\n",
        "openai.api_key = secret  #never share your private API keys with the world! read them from enviroment or private text file"
      ],
      "metadata": {
        "id": "DVAjoD-bb4Vf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing the API\n",
        "\n",
        "https://github.com/openai/openai-python - documents the changes\n",
        "\n",
        "Since this is cutting edge research, API changes quite often.\n",
        "\n",
        "Expect things to stabilize in a few years."
      ],
      "metadata": {
        "id": "NUcrvN9ivPWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    # api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "    api_key=secret, # instead of accessing my api key from my os, i use the key i pasted in to secret\n",
        "    # again secret is just a string but do not write it directly here!!\n",
        ")"
      ],
      "metadata": {
        "id": "aFmfUYbFvmWb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Say this is a test but translate into Latvian\", # so this is your prompt\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\", # this is among the cheapest of the models similar to the free version on ChatGPT\n",
        "    # there are more options but we will stick with basics\n",
        ")\n",
        "# so with this example we made a call to OpenAI using the API key"
      ],
      "metadata": {
        "id": "HkelQakVvedo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can get the json response of everything the model provided\n",
        "chat_completion.json()"
      ],
      "metadata": {
        "id": "CP9VHpHevzmc",
        "outputId": "b7b7a04e-3841-4633-f86f-5be88e6ecff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3186247119.py:2: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  chat_completion.json()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"id\":\"chatcmpl-Co7NUSYYdgDa634PpjNMolpfSQeJU\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"≈†is ir tests.\",\"refusal\":null,\"role\":\"assistant\",\"annotations\":[],\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1766060796,\"model\":\"gpt-3.5-turbo-0125\",\"object\":\"chat.completion\",\"service_tier\":\"default\",\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":6,\"prompt_tokens\":18,\"total_tokens\":24,\"completion_tokens_details\":{\"accepted_prediction_tokens\":0,\"audio_tokens\":0,\"reasoning_tokens\":0,\"rejected_prediction_tokens\":0},\"prompt_tokens_details\":{\"audio_tokens\":0,\"cached_tokens\":0}}}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now to get just the answer in text form\n",
        "chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "1L3M1pPewD5Q",
        "outputId": "e9ba0844-dffb-4b8f-fde9-e72107b9775c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'≈†is ir tests.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chat_completion.choices) # how many choices do we have?"
      ],
      "metadata": {
        "id": "am4dU3p9wvqr",
        "outputId": "ad589ece-a483-4e51-b83a-27e45d003c19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jd-lg20kyCZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automating Sentiment Analysis\n",
        "\n",
        "To avoid writing all the boileplate code by hand, instead I will write a function that combines all the required code and I can call the function instead, whenever I need sentiment analysis."
      ],
      "metadata": {
        "id": "J3BljXVmHLYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getSentiment(prompt, client=client, sentiments=(\"positive\",\"neutral\",\"negative\"), model=\"gpt-3.5-turbo\", max_prompt=200):\n",
        "    sentiment_text = \",\".join(sentiments)  # I add all the sentiments in a string separated by comma\n",
        "    prompt=f\"Social media post: \\\"{prompt[:max_prompt]}\\\"Sentiment ({sentiment_text}):\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "uqSeqhEcyFY-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"I really like bread and circuses\")\n",
        "# depending on temperature setting we might get different answers"
      ],
      "metadata": {
        "id": "gwvz0Tbvyn2z",
        "outputId": "373a344e-7ec9-4b40-bc73-337642c095b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Neutral'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Where do I begin? This is a brand new 4K scan from the original negative of the movie with an added HDR10 & Dolby Vision HDR grading, which looks fantastic!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yrZMnvmhnx1W",
        "outputId": "2c58ee38-b41f-45b5-d308-00730f3ce5ad"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Positive'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Man patik alus\") # I like beer in Latvian"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Esn2RaidoN_n",
        "outputId": "c904e8c5-e771-488c-c4fe-93e11672cefb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Neutral'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Man nepatƒ´k slidenas ielas\") #I do not like slippery streets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0ZdW9uicoWXh",
        "outputId": "e83ac564-80f6-428d-81da-9e52be0d9c42"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using this custom function you can pass in your own sentiments - for example in Latvian\n",
        "getSentiment(\"Man nepatƒ´k slidenas ielas\", sentiments=[\"pozitƒ´vs\", \"neitrƒÅls\", \"negatƒ´vs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vj0lBNvfpDk4",
        "outputId": "22c5be4d-eb0b-40fc-948f-007de11d9c8e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negatƒ´vs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getSentiment(\"Man patik alus\", sentiments=[\"pozitƒ´vs\", \"neitrƒÅls\", \"negatƒ´vs\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I1uBEGwQpShR",
        "outputId": "a4733922-799f-4cfc-a7b3-05b3d1ae5dec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pozitƒ´vs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt creation\n",
        "\n",
        "So basica idea is we provide a sort of answer key when we need sentiment analysis. We ended our prompt with possible answers and LLM gave us one of them."
      ],
      "metadata": {
        "id": "YRWwxTzvIw2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# so this is our own ChatGPT basically, except we can adjust more parameters\n",
        "def getAnswer(prompt, client=client, model=\"gpt-3.5-turbo\", max_prompt=500):\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt[:max_prompt],\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "FWvsAxxhJE-J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getAnswer(\"What is the capital of Latvia?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HPIGW71mJaUy",
        "outputId": "34468687-09c9-4f3d-f023-aee263ce62f0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Riga'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# of course LLMs have a infamous tendendcy to hallucinate especially when the model does not have the full answer.\n",
        "# One possible test to evaluate is to ask a question where the answer is between ranges of some information it already knows\n",
        "getAnswer(\"Who was president of Latvia in 1926?\") # key being that 1926 was a year President of Latvia did not do anything special"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vmMYuABQJjZQ",
        "outputId": "1dee9098-7c9d-4edd-a498-a56dcde1ba3e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gustavs Zemgals was the president of Latvia in 1926.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getAnswer(\"Who was president of Latvia in 1926? Write a short paragraph on this president\") # key being that 1926 was a year President of Latvia did not do anything special"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "-IPEVFv0KB8q",
        "outputId": "f8785823-d0f8-45a4-9865-421d2d86a357"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Gustavs Zemgals was the president of Latvia in 1926. A prominent lawyer and politician, Zemgals was known for his commitment to upholding the principles of democracy and the rule of law. During his presidency, he worked to strengthen Latvia's economy, improve social welfare, and promote diplomatic relations with other countries. Zemgals was a respected leader who strived to ensure stability and prosperity for the people of Latvia during a time of political uncertainty and economic challenges.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### getAnswer as analogous to ChatGPT interface\n",
        "\n",
        "So now if you had say 100 documents to analyse you could write a loop and pass this function 100 times."
      ],
      "metadata": {
        "id": "rS-2_WCJKSes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getMovieEmoji(movie_title,client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"\"\"Back to Future: üë®üë¥üöóüïí\n",
        "    Batman: ü§µü¶á\n",
        "    Transformers: üöóü§ñ\n",
        "    Wonder Woman: üë∏üèªüë∏üèºüë∏üèΩüë∏üèæüë∏üèø\n",
        "    Winnie the Pooh: üêªüêºüêª\n",
        "    The Godfather: üë®üë©üëßüïµüèª‚Äç‚ôÇÔ∏èüë≤üí•\n",
        "    Game of Thrones: üèπüó°üó°üèπ\n",
        "    {movie_title}: \"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "SWXEr_F9qbtH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iN-UHwVgK-1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(getMovieEmoji(\"The Bourne Conspiracy\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XGO_WoNq6eM",
        "outputId": "451a8146-b787-4b31-d9e0-e2ddc012b29b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üïµÔ∏è‚Äç‚ôÇÔ∏èüíºüî´üïµÔ∏è‚Äç‚ôÄÔ∏èüèÉ‚Äç‚ôÇÔ∏èüîç\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getMovieEmoji(\"Top Gun: Maverick\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqqJRsogLgvy",
        "outputId": "c8f1d502-fd5e-4cec-a8bb-20f18a2d3c0b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ©Ô∏èüë®‚Äç‚úàÔ∏èüî•\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getMovieEmoji(\"Frozen 2\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaIIowiALnnO",
        "outputId": "d2125e45-4daa-4979-993d-dca7793e5675"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üëß‚ùÑÔ∏èüë∏üßäü¶åüå¨Ô∏èüèîÔ∏è\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key takeaway - answers are not deterministic\n",
        "\n",
        "Notice how multiple calls can return different answers, it is due to the temperature setting that slightly changes the answers.\n",
        "\n",
        "Model stays the same but there is an element of slight randomness present.\n",
        "\n",
        "So when you see some amazing LLM examples in action you have to ask how many tries did it take to generate them. Case in point recent Google Gemini Demo presentation."
      ],
      "metadata": {
        "id": "HKSOVVzELvon"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEDcNi-jLudY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.esrb.org/\n",
        "def getGameRating(movie_text,client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"\"\"\"Provide an ESRB rating for the following text:\n",
        "    {movie_text}\n",
        "    ESRB rating:\"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_tokens=60,\n",
        "        temperature=0.7 # temperature ranges from 0 to 1\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "7pIUj35jsAdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getGameRating(\"The game opens with a screen in a dark and stormy night. Five gamblers sit around a fire in a dark forest.\")\n",
        "# so one of the issues with LLMs is that you need to provide enought context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-NDcBK7jNJZW",
        "outputId": "12cec7da-b9cc-4faa-c1c1-8f6a7d2b77c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The ESRB rating for the given text would likely be \"Teen\" (T) due to the dark and potentially intense atmosphere depicted in the opening scene.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getMovieRating(movie_text,client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"\"\"\"Provide an MPA rating to the movie based on following description:\n",
        "    {movie_text}\n",
        "    MPA rating:\"\"\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        max_tokens=60,\n",
        "        temperature=0.7 # temperature ranges from 0 to 1\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "GbR698W1ub-_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getMovieRating(\"They say all happy families are alike but all unhappy families are different in their own way\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IaObBgequjrG",
        "outputId": "2b1f515c-1e9e-4adf-af97-3d09efaa0abd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PG-13'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getStudyNotes(subject, client=client, model=\"gpt-3.5-turbo\"):\n",
        "    prompt=f\"What are some key points I should know when studying {subject} Provide at least five key points\\n\\n1.\"\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model,\n",
        "        # using default for now, you can play around with some of the parameters\n",
        "        # temperature=1,\n",
        "        # max_tokens=64,\n",
        "        # top_p=1.0,\n",
        "        # frequency_penalty=0.0,\n",
        "        # presence_penalty=0.0\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "Ti9Vdw62u2AY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(getStudyNotes(\"Riga\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Aqk-WJTvZh5",
        "outputId": "3e61912f-6fba-4400-ded6-fef34f577931"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Riga is the capital city of Latvia, located on the banks of the Daugava River in the Baltic region. It is the largest city in the country and serves as an important economic, political, and cultural center.\n",
            "\n",
            "2. Riga is known for its well-preserved medieval old town, which is a UNESCO World Heritage site. The old town is home to numerous historic buildings, including Riga Cathedral, the House of the Blackheads, and the Swedish Gate.\n",
            "\n",
            "3. Riga is a multicultural city with a diverse population, including Latvians, Russians, Belarusians, and Ukrainians. This diversity is reflected in the city's architecture, cuisine, and cultural traditions.\n",
            "\n",
            "4. Riga has a vibrant arts and cultural scene, with numerous museums, theaters, and art galleries showcasing both traditional and contemporary Latvian art. The city also hosts several major cultural events, such as the Riga Opera Festival and the Latvian Song and Dance Festival.\n",
            "\n",
            "5. Riga is a popular tourist destination, known for its charming cobbled streets, beautiful parks, and vibrant nightlife. Visitors can enjoy exploring the city's many historical sites, relaxing in its green spaces, and sampling traditional Latvian cuisine in its many restaurants and cafes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's try it with latest and gratest 5.2\n",
        "print(getStudyNotes(\"Riga\", model=\"gpt-5.2\"))"
      ],
      "metadata": {
        "id": "DxLncRpfcVMK",
        "outputId": "ea808121-406c-4bf4-e166-0d5ac0859599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **Geography & setting**: Riga is the capital of Latvia, located on the **Daugava River** near the **Gulf of Riga** (Baltic Sea). Its position made it a major **trade and transport hub** in Northern Europe.\n",
            "\n",
            "2. **Historical development**: Founded in **1201** (traditionally associated with Bishop Albert), Riga grew quickly as a regional center. It later became part of major powers over time, including the **Polish‚ÄìLithuanian Commonwealth**, **Sweden**, the **Russian Empire**, and the **Soviet Union**, before Latvia restored independence in **1991**.\n",
            "\n",
            "3. **Hanseatic League & trade**: Riga was an important member of the **Hanseatic League** (a medieval trade network). This shaped its economy, architecture, and multicultural character through merchants and international connections.\n",
            "\n",
            "4. **Architecture & Old Town (Vecrƒ´ga)**: Riga is famous for its **well-preserved medieval Old Town**, a **UNESCO World Heritage** area, and for having one of Europe‚Äôs richest collections of **Art Nouveau (Jugendstil)** architecture (especially in the early 1900s).\n",
            "\n",
            "5. **Culture & language**: The official language is **Latvian**, and the city has a historically diverse population (including significant **Russian-speaking** communities). Riga is a key center for Latvian culture, music, and national traditions.\n",
            "\n",
            "6. **Modern Riga & economy**: Today Riga is Latvia‚Äôs main **economic and educational** center, with strengths in services, logistics, tech/startups, and tourism. It also hosts major institutions, universities, and cultural events.\n",
            "\n",
            "If you tell me whether you‚Äôre studying Riga for **history**, **geography**, **politics**, or **travel/culture**, I can tailor the key points to your course.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i am using print because this answer includes newlines\n",
        "study_notes_on_dd = getStudyNotes(\"Digital Discourse\")\n",
        "print(study_notes_on_dd)\n",
        "# note how 1. is missing from the answer?\n",
        "# why?\n",
        "# because I already provide 1. at the end of my prompt\n",
        "# remember LLMs are just word prediction machines (with some useful side effects)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FERP6tSvO9py",
        "outputId": "1e9ae1ec-df1f-442d-8932-6cda9c8cc3c6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Digital discourse refers to communication and interaction that takes place through digital platforms such as social media, online forums, and messaging apps.\n",
            "   \n",
            "2. It is important to consider the context and medium in which digital discourse is taking place, as this can greatly influence the tone, style, and content of communication.\n",
            "   \n",
            "3. Social media has revolutionized digital discourse by making it easier for people to share their thoughts and opinions with a wide audience, but it has also raised concerns about privacy, misinformation, and online bullying.\n",
            "   \n",
            "4. The use of emojis, GIFs, hashtags, and other forms of visual and textual shorthand are common in digital discourse and can convey subtle nuances of meaning and emotion.\n",
            "   \n",
            "5. Digital discourse can be both empowering and challenging, as it allows for greater freedom of expression and connection with others, but also presents risks of miscommunication, harassment, and data privacy concerns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now i can save my notes to text file\n",
        "with open(\"study_notes.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(study_notes_on_dd)"
      ],
      "metadata": {
        "id": "lXFUi6UFPKOc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's do the same with 5.2"
      ],
      "metadata": {
        "id": "hUi38vuvcld0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion on use of LLMs in Digital Discourse\n",
        "\n",
        "So LLMs can aid in discourse analysis with what:\n",
        "\n",
        "* Automated Text Analysis\n",
        "* Sentiment Analysis\n",
        "* Topic Modeling\n",
        "* Translation\n",
        "* Context Analysis\n",
        "* Cross-cultural Analyss\n",
        "* Summarization\n",
        "* Bias detection"
      ],
      "metadata": {
        "id": "f1HvTyjRPvp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notes = [getStudyNotes(\"Sentiment Analysis\") for _ in range(5)]  # i ran the same query 5 times, so text completion will be different each tie\n",
        "print(notes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKmTUYYcvkuR",
        "outputId": "0afb34d8-8cd1-4607-aea3-87e00b7e4afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' Keywords and phrases can be negative even when they don‚Äôt seem to be\\n2. Popular sentiment analysis is not always the right analysis\\n3. Slang and capitals can make sentences seem negative', ' Semantria tracks sentiment globally by applying sentiment analysis models to content expressed in multiple languages.\\n2. Semantria sentiment analysis models come in three different methods which are Query Based, Feed Based, and Sentiment Match.\\n3. With Query Based sentiment analysis you can input a phrase or sentence for Semantria', \" Find out what is interesting about your business\\n2. Focus on your buying process\\n\\n3. Reduce capital inventory\\n4. Have the temperament of an artists\\n\\nWhat key points do you think I should know for this topic?\\n\\nYou shouldn't study how to do sentiment analysis or focus on this topic if\", ' Authors generally use sentiment words to convey approval versus a negative sentiment\\n a group holds about a person, company, issue, or life events.\\n2. Automated sentiment analysis is a technique for analyzing texts to make scientifically-based assessments on whether a text is of a broadly positive or negative tone\\n3. Classifiers', ' The sentiment of a sentence should take into account the sentiment of the words in it. So, for example, take the sentence ‚ÄúIt was great to get to study with you!‚Äù. By taking the sentiment of the words ‚Äústudy‚Äù, ‚Äúget‚Äù, and ‚Äúglad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "digital_discourse_notes = [getStudyNotes(\"Digital Discourse\") for _ in range(5)]\n",
        "for note in digital_discourse_notes:\n",
        "  print(note)\n",
        "  print(\"=\"*40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u11YHzV5wJ2f",
        "outputId": "314f598a-b022-49c9-a7b0-6d1c854cda8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " There is a relationship between the word used and the identity of the individual speaker.\n",
            "\n",
            "2. Gender can also change depending on the audience.\n",
            "\n",
            "3. Tone serves as a crucial component for comprehension.\n",
            "\n",
            "4. Slang and jargon differ from culture to culture and language to language\n",
            "\n",
            "5.\n",
            "========================================\n",
            "Doesn't exist in a bubble so we need to pay attention to the framing and the understanding and assumptions around the discourse\n",
            "\n",
            "2.It can be participatory and evolving\n",
            "\n",
            "3.We need to study the social and political dimensions\n",
            "\n",
            "4.Water cooler talk\n",
            "\n",
            "5. A means of both self\n",
            "========================================\n",
            " Digital Discourse includes the many locations of the discussion, such as a blog, a message board, a chatroom, a wiki, a MySpace page, a Facebook page, a YouTube video, etc.\n",
            "A) Different people have different digital practices when it comes to culture\n",
            "\n",
            "2. There are \"four\n",
            "========================================\n",
            " Textual and Visual 2. Readers and Writers 3. Poetic Knowledge and Poetic Practice 4. Technical and Cultural 5. Historical and Scientific 6. Analysis and Interdisciplinarity\n",
            "\n",
            "What is an example of a semantic textual bias in a document?\n",
            "\n",
            "A semantic textual bias in a document is it reads\n",
            "========================================\n",
            " The value of the status quo\n",
            "\n",
            "2. Stance\n",
            "\n",
            "3. The one who doesn't know\n",
            "\n",
            "4. Tools of language\n",
            "\n",
            "5. Summary\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getEssayOutline(subject):\n",
        "  response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=f\"Create an outline for an essay about {subject}:\\n\\nI: Introduction\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=60,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0\n",
        ")\n",
        "  return response[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "u08bJVfkw3e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(getEssayOutline(\"Julius Cesar\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVrdpD2txQlc",
        "outputId": "9db87882-1893-4ba5-f0cf-5301cd9f8bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "II: Julius Cesar\n",
            "\n",
            "III: Family background\n",
            "\n",
            "IIII: Early life\n",
            "\n",
            "IIII: Civil service\n",
            "\n",
            "V: Cesar and Crassus\n",
            "\n",
            "VI: Cesar and Pompey\n",
            "\n",
            "VII: Cesar and the provinces\n",
            "\n",
            "VIII:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getEssayOutline(\"Tourism in Latvia\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMqJRBF9yCa9",
        "outputId": "55146e99-2f98-49e0-ab20-926a1fd6f3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "- Tourism in Latvia.\n",
            "\n",
            "II: What are the main development directions of the tourism business in Latvia?\n",
            "\n",
            "- Growth of the tourism industry.\n",
            "\n",
            "- The tourism industry as one of the most dynamic economic sectors in Latvia.\n",
            "\n",
            "- Importance of the tourism industry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getHorrorStory(topic):\n",
        "  response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=f\"Topic: Breakfast\\nTwo-Sentence Horror Story: He always stops crying when I pour the milk on his cereal. I just have to remember not to let him see his face on the carton.\\n###\\nTopic: {topic}\\nTwo-Sentence Horror Story:\",\n",
        "  temperature=0.5,\n",
        "  max_tokens=60,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.5,\n",
        "  presence_penalty=0.0,\n",
        "  stop=[\"###\"]\n",
        ")\n",
        "  return response[\"choices\"][0][\"text\"]"
      ],
      "metadata": {
        "id": "wVXXn0Qdy8AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(getHorrorStory(\"snow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUv_ZK8AzLXE",
        "outputId": "ec242bf5-d10e-4f88-dc89-22d5909f3dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I was walking home from work when I realized that I was the only one on the sidewalk. A few minutes later, I saw a snowplow coming down the road. I waved to get its attention, but it just kept on going.\n",
            "I don't know what happened to everyone else.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getHorrorStory(\"Christmas\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCdaEt7XzYjI",
        "outputId": "25c8deba-7d0a-42d4-e1b1-286a57e4de58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The real Santa Claus was too fat to fit down the chimney, so he left a note saying he'd be back next year.\n",
            "Two-Sentence Horror Story: I think Santa Claus is going to kill me.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# once I have a list of some texts to analyzie\n",
        "# i can simply loop over them and clal my getSentiment function for each text/document\n",
        "my_tweets = [\"In October main exports partners were Lithuania, Estonia, Germany and United Kingdom. The main import partners were Lithuania, Russian Federation, Poland and Germany.\"\n",
        ",\"In October 2021 the foreign trade turnover of Latvia amounted to ‚Ç¨ 3.39 billion, which at current prices was 23.5% larger than a year ago.Exports value of goods was ‚¨ÜÔ∏è17.8% higher, but imports value of goods ‚¨ÜÔ∏è28.8% higher. \"]\n",
        "\n",
        "my_tweet_sentiments = [getSentiment(tweet) for tweet in my_tweets]\n",
        "my_tweet_sentiments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaiMIPijzuzr",
        "outputId": "0aa4df47-0c80-41ac-c0bd-d9d237975230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Positive', ' 0']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternatives to OpenAI API\n",
        "\n",
        "These days you have many many alternative LLM providers, such as Gemini from Google, Claude from Anthropic, Mistral, DeepSeek and many more\n",
        "\n",
        "You could apply for API keys at individual ones, but my recommendation is to use an aggregrator.\n",
        "\n",
        "I personally use https://openrouter.ai which let's me use SAME API to access pretty much any Large Language Model in the world - at least 400 as of last count.\n",
        "\n",
        "Downside you pay about 5% premium for this convenience.\n",
        "\n",
        "Sometimes this is actually unavoidable if you want to connect to some APIs as some providers are very hard to get a paid account and set it up, even Google makes things too difficult at times.\n",
        "\n",
        "Another advantage of Openrouter.ai API keys is that you can set a SPENDING LIMIT on how much you want that key to allow.\n",
        "\n",
        "This is crucial if you are worried of letting some script run wild and rack up huge bills. :)\n",
        "\n"
      ],
      "metadata": {
        "id": "W-pWvj1qdT7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Premade Jupyter Notebook for Batch processing\n",
        "\n",
        "If you want to avoid coding your own notebook, you can use\n",
        "\n",
        "While working at various projects at National Library of Latvia, I've had colleagues ask for batch processing notebook:\n",
        "\n",
        "You are also free to use it - just use your own openrouter.ai key:\n",
        "\n",
        "https://colab.research.google.com/github/LNB-DH/PublicTools/blob/main/notebooks/llm/batch_processing.ipynb\n",
        "\n"
      ],
      "metadata": {
        "id": "dF63GSJieM12"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4jyYXiueJlc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}